<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>t-SNE</title>
    <link href="/2021/02/05/t-SNE/"/>
    <url>/2021/02/05/t-SNE/</url>
    
    <content type="html"><![CDATA[<h1 id="无监督学习之“-t分布随机近邻嵌入t-SNE算法”"><a href="#无监督学习之“-t分布随机近邻嵌入t-SNE算法”" class="headerlink" title="无监督学习之“ t分布随机近邻嵌入t-SNE算法”"></a>无监督学习之“ t分布随机近邻嵌入t-SNE算法”</h1><p>引子：跟着“宝可梦”老师，台大李宏毅机器学习课程，我从PCA来到了t-SNE的领域。</p><p>开始正文前，先补充一些关于流形学习的概念和分类：</p><h2 id="1-流形学习"><a href="#1-流形学习" class="headerlink" title="1. 流形学习"></a>1. 流形学习</h2><p>流形学习方法(Manifold Learning)，简称流形学习，自2000年在著名的科学杂志《Science》被首次提出以来，已成为信息科学领域的研究热点。在理论和应用上，流形学习方法都具有重要的研究意义。</p><p>假设数据是均匀采样于一个高维欧氏空间中的低维流形，流形学习就是从高维采样数据中恢复低维流形结构，即找到高维空间中的低维流形，并求出相应的嵌入映射，以实现维数约简或者数据可视化。它是从观测到的现象中去寻找事物的本质，找到产生数据的内在规律。</p><p>简单地理解，流形学习方法可以用来对高维数据降维，如果将维度降到2维或3维，我们就能将原始数据可视化，从而对数据的分布有直观的了解，发现一些可能存在的规律。</p><p>可以将流形学习方法分为线性的和非线性的两种，线性的流形学习方法如我们熟知的主成份分析（PCA），非线性的流形学习方法如等距映射（Isomap）、拉普拉斯特征映射（Laplacian eigenmaps，LE）、局部线性嵌入(Locally-linear embedding，LLE)。关于后几个方法，可参看<a href="https://zhuanlan.zhihu.com/p/40214106">流形学习概述 - SIGAI的文章 - 知乎</a></p><p><img src="https://i.loli.net/2021/02/05/OMgUxNEjzw3HVlX.jpg" alt="数据降维.jpg"></p><p>流形学习（manifold learning）假设数据在高维空间的分布位于某一更低维的流形上，基于这个假设来进行数据的分析。对于降维，要保证降维之后的数据同样满足与高维空间流形有关的几何约束关系。除此之外，流形学习还可以用实现聚类，分类以及回归算法。</p><h2 id="2-t-SNE（t-distributed-stochastic-neighbor-embedding"><a href="#2-t-SNE（t-distributed-stochastic-neighbor-embedding" class="headerlink" title="2. t-SNE（t distributed stochastic neighbor embedding)"></a>2. t-SNE（t distributed stochastic neighbor embedding)</h2><p>t-SNE模型是通过SNE演变而来，它跟kmeans等不同，他不能通过训练得到一些东西之后再用于其它数据（比如kmeans可以通过训练得到k个点，再用于其它数据集，而t-SNE只能单独的对数据做操作，也就是说他只有fit_transform，而没有fit操作）。</p><p>SNE是通过仿射(affinitie)变换将数据点映射到概率分布上，主要包括两个步骤：</p><ul><li>SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。</li><li>SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似。</li></ul><p>由于SNE存在“crowding problem”(拥挤问题)。因此Hinton等人又提出了t-SNE的方法。与SNE不同，主要如下:</p><ul><li>使用对称版的SNE，简化梯度公式</li><li>低维空间下，使用t分布替代高斯分布表达两点之间的相似度</li></ul><p>SNE是先<strong>将欧几里得距离转换为条件概率来表达点与点之间的相似度</strong>。具体来说，给定一个N个高维的数据$x_1,… ,x_N$,</p><p>SNE首先是计算概率$P_{ij}$，正比于$x_i$和$x_j$之间的相似度。即</p><script type="math/tex; mode=display">P_{j|i}=\cfrac{exp(-||x_i-x_j||^2)/(2\sigma_i^2)}{\sum_{k\ne i}exp(-||x_i-x_k||^2)/(2\sigma_i^2)}\tag 1</script><p>其中$p_{j|i}$代表$x_j$是$x_i$的”邻居“的概率，值越大，说明两者越”临近“；$\sigma_i$是$x_i$最邻近的N个点的方差值，其中N是一个超参数，定义为perplexity困惑度的概念。<strong>SNE对困惑度的调整比较有鲁棒性，通常选择5-50之间</strong>，给定之后，使用二分搜索的方式寻找合适的$\sigma$。</p><p>对于低维数据集中的两个点$y_i$和$y_j$，定义公式如下：</p><script type="math/tex; mode=display">q_{j|i}=\cfrac{exp(-||y_i-y_j||^2)}{\sum_{k\ne i}exp(-||y_i-y_k||^2)}\tag 2</script><p>$q_{j|i}$的意义和$p_{j|i}$的意义相同，低维数据最开始是随机生成，后面根据$p_{j|i}$和$q_ {j|i}$定义loss函数，通过调整${y_1, y_2, y_3,…}$的值最小化loss函数，由此生成最终的低维数据。</p><p>定义KL散度Kullback-Leibler divergence 作为loss函数</p><script type="math/tex; mode=display">C=\sum_iKL(P_i||Q_i)=\sum_i\sum_jp_{j|i}\log\cfrac{p_{j|i}}{q_{j|i}}\tag 3</script><p>t-SNE用联合概率分布替代公式(1)中的条件概率分布，则loss函数变为</p><script type="math/tex; mode=display">C=\sum_iKL(P||Q)=\sum_i\sum_jp_{i,j}\log\cfrac{p_{ij}}{q_{ij}}\tag 4</script><p>这里的$p_{ii}$,$q_{ii}$为零，即为对称SNE，假设了对于任意$i$，$p_{ij}=p_{ji}$,$q_{ij}=q_{ji}$</p><p>对称SNE实际上在高维度下，另外一种减轻”拥挤问题”的方法：在高维空间下，使用高斯分布将距离转换为概率分布，在低维空间下，使用更加偏重长尾分布的方式来将距离转换为概率分布，使得高维度下中低等的距离在映射后能够有一个较大的距离。使用了t分布之后的q变化，如下:</p><script type="math/tex; mode=display">q_{ij}=\cfrac{(1+||y_i-y_j||^2)^{-1}}{\sum_{k\ne i}(1+||y_i-y_k||^2)^{-1}}\tag 5</script><p>此外，t分布是无限多个高斯分布的叠加，计算上不是指数的，会方便很多。优化的梯度如下:</p><script type="math/tex; mode=display">\cfrac{\delta C}{\delta {y_i}}=4\sum_j(p_{ij}-q_{ij})(y_i-y_j)(1+||y_i-y_j||^2)^{-1}\tag 6</script><p><img src="https://i.loli.net/2021/02/05/NTv9143BeUM57YE.png" alt="sne_norm_t_dist_cost.png"></p><p>t-SNE的有效性，也可以从上图中看到：横轴表示距离，纵轴表示相似度, 可以看到，对于较大相似度的点，t分布在低维空间中的距离需要稍小一点；而对于低相似度的点，t分布在低维空间中的距离需要更远。这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。</p><p>总结一下，t-SNE的梯度更新有两大优势：</p><ul><li>对于不相似的点，用一个较小的距离会产生较大的梯度来让这些点排斥开来。</li><li>这种排斥又不会无限大(梯度中分母)，避免不相似的点距离太远。</li></ul><p>主要不足有四个:</p><ul><li>主要用于可视化，很难用于其他目的。比如测试集合降维，因为他没有显式的预估部分，不能在测试集合直接降维；比如降维到10维，因为t分布偏重长尾，1个自由度的t分布很难保存好局部特征，可能需要设置成更高的自由度。</li><li>t-SNE倾向于保存局部特征，对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间</li><li>t-SNE没有唯一最优解，且没有预估部分。如果想要做预估，可以考虑降维之后，再构建一个回归方程之类的模型去做。但是要注意，t-SNE中距离本身是没有意义，都是概率分布问题。</li><li>训练太慢。有很多基于树的算法在t-SNE上做一些改进。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html">t-SNE完整笔记</a></li><li><a href="https://zhuanlan.zhihu.com/p/102668730">流形学习-高维数据的降维与可视化 - 屁屁烫的文章 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/40214106">流形学习概述 - SIGAI的文章 - 知乎</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>unsupervised learning</tag>
      
      <tag>dimension reduction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PCA</title>
    <link href="/2021/01/29/PCA/"/>
    <url>/2021/01/29/PCA/</url>
    
    <content type="html"><![CDATA[<h1 id="无监督学习之“主成分分析PCA算法“"><a href="#无监督学习之“主成分分析PCA算法“" class="headerlink" title="无监督学习之“主成分分析PCA算法“"></a>无监督学习之“主成分分析PCA算法“</h1><p><strong>主成分分析（principal component analysis，PCA）</strong>是一种常用的<strong>无监督学习方法</strong>，PCA利用<strong>正交变换</strong>把由<strong>线性相关</strong>变量表示的观测数据转换为少数几个由<strong>线性无关</strong>变量表示的数据，线性无关的变量称为主成分。这里，<strong>主成分（principal component，PC）</strong>的个数一般小于原始数据变量个数，因此主成分分析属于<strong>降维分析方法(Dimension reduction)</strong>。</p><p><strong>PCA主要应用场景</strong>：数据压缩；消除冗余；消除数据噪声；数据降维，可视化。</p><p><strong>理论基础</strong>：最大投影方差、最小投影距离和坐标轴相关度等。</p><p><strong>直观理解</strong>：找出数据里最主要的成分，代替原始数据并使损失尽可能的小。</p><h2 id="1-相关线性代数知识回顾"><a href="#1-相关线性代数知识回顾" class="headerlink" title="1. 相关线性代数知识回顾"></a>1. 相关线性代数知识回顾</h2><p>此处只简单罗列几个相关的线性代数知识点，具体可以参考<a href="https://zhuanlan.zhihu.com/p/74833836">机器学习01—PCA数学推导</a>、<a href="https://zhuanlan.zhihu.com/p/343182129">算法理论02 主成分分析PCA</a> 。</p><h3 id="1-1-内积"><a href="#1-1-内积" class="headerlink" title="1.1 内积"></a>1.1 内积</h3><script type="math/tex; mode=display">Z=WX\tag 1</script><p>其中$W$中的每一个分量向量的模为1，即$||w_i||_2=1$。</p><h3 id="1-2-基变化的矩阵化"><a href="#1-2-基变化的矩阵化" class="headerlink" title="1.2 基变化的矩阵化"></a>1.2 基变化的矩阵化</h3><p>如果我们有m个N维向量，想将其变化为有r个N维向量表示新空间中，那么首先将r个基按行组成矩阵$W$，然后将向量按列组成矩阵$X$，矩阵$W$和$X$相乘的结果就是变换的结果，数学表达式为：</p><script type="math/tex; mode=display">{\begin{pmatrix}w_1\\w_2\\\vdots \\w_r\end{pmatrix}}(x_1,x_2,\cdots,x_m)={\begin{pmatrix}w_1x_1 & w_1x_2 & \cdots & w_1x_m\\w_2x_1 & w_2x_2 & \cdots & w_2x_m\\\vdots & \vdots & \ddots & \vdots\\w_rx_1 & w_rx_2 & \cdots & w_rx_m\\\end{pmatrix}}\tag 2</script><p>根据上式可以知道，r的维度决定了变换后数据的维度，若r小于m，我们可以实现降维的操作，也就是说<strong>我们可以将数据变换到低维的空间中，变换后的维度取决于基的数量</strong>。</p><h3 id="1-3-方差"><a href="#1-3-方差" class="headerlink" title="1.3 方差"></a>1.3 方差</h3><p>数学中方差用于衡量数据的离散程度:</p><script type="math/tex; mode=display">var(x)=\cfrac{1}{m}\sum_{i=1}^m(x_i-\bar{x})^2\tag 3</script><p>其中</p><script type="math/tex; mode=display">\bar{x}=\cfrac{1}{m}\sum_{j=1}^mx_j\tag 4</script><h3 id="1-4-协方差"><a href="#1-4-协方差" class="headerlink" title="1.4 协方差"></a>1.4 协方差</h3><p>数学中协方差用于衡量数据间的相关性，如果协方差为0，表示数据完全独立，</p><script type="math/tex; mode=display">S=\cfrac{1}{m}\sum_{i=1}^m(x-\bar{x})(x-\bar{x})^T\tag 5</script><h2 id="2-优化目标的引出"><a href="#2-优化目标的引出" class="headerlink" title="2.  优化目标的引出"></a>2.  优化目标的引出</h2><h3 id="2-1-方差最大化"><a href="#2-1-方差最大化" class="headerlink" title="2.1 方差最大化"></a>2.1 方差最大化</h3><p>降维问题的优化目标：<strong>将一组N维向量降为R维（0&lt;R&lt;N），其目标是选择R个单位（模为1）的正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，字段间方差尽可能大，在正交的约束下，取最大的R个方差。</strong></p><h3 id="2-2-投影距离最小化"><a href="#2-2-投影距离最小化" class="headerlink" title="2.2  投影距离最小化"></a>2.2  投影距离最小化</h3><p>利用拉格朗日乘子法得到</p><script type="math/tex; mode=display">XX^TW=\lambda W\tag 6</script><p>因此$W$是由对应于特征值$\lambda$的特征向量组成的矩阵。</p><h3 id="2-3-协方差矩阵对角化"><a href="#2-3-协方差矩阵对角化" class="headerlink" title="2.3 协方差矩阵对角化"></a>2.3 协方差矩阵对角化</h3><p>字段内方差最大，字段间协方差为0，这等价于将协方差矩阵对角化，即化为除对角线外的其它元素均为0，并且在对角线上将元素从大到小从上往下依次排列，我们只要取前R个就达到了优化目标！</p><p>设原始数据矩阵$X$对应的协方差矩阵为S，而$W$是一组基按行组成的矩阵，设$Z=WX$，则$Z$为$X$对$W$做基变换后的数据，设$Z$的协方差矩阵为D，我们推导一下D与S的关系：</p><script type="math/tex; mode=display">D=\sum(z-\bar{z})(z-\bar{z})^T=W(\sum(x-\bar{x})(x-\bar{x})^T)W^T=WSW^T\tag 7</script><p><strong>寻找一个矩阵$W$，满足$WSW^T$是一个对角矩阵，并且对角元素从上到下从大到小依次排列，那么$W$的前R行就是要寻找的基，用$W$的前R行组成的矩阵乘以原始矩阵$X$就使得$X$从N维降到了R维并且可以尽可能的保存原有数据的信息</strong>。</p><p>由上面的推导可知，协方差矩阵S是一个对称矩阵，根据对称矩阵的两个重要性质可知：一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为 $E=(e^1,e^2,\cdots,e^n)$我们将其按列组成矩阵：则</p><script type="math/tex; mode=display">E^TSE={\begin{pmatrix}\lambda_1&&&\\&\lambda_2&&\\&&\ddots&\\&&&\lambda_n\end{pmatrix}}\tag 8</script><p>到这里我们就已经找到了我们需要的矩阵$W=E^T$,即</p><script type="math/tex; mode=display">XX^TW=\lambda W=\lambda E^T\tag 9</script><p>因此$D$是对角矩阵Diagonal matrix。</p><p>$W$是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是S的一个特征向量，如果设$W$按照式(7)中特征值的大小从大到小将特征向量从上往下排列，则用$W$的前K行组成的矩阵乘以原始数据矩阵$X$，就得到了我们需要的矩阵$Z$，这就是PCA部的数学推导！</p><h2 id="3-PCA-的算法步骤"><a href="#3-PCA-的算法步骤" class="headerlink" title="3. PCA 的算法步骤"></a>3. PCA 的算法步骤</h2><p>设有m条N维数据：<br>①将原始数据按列组成N行m列矩阵$X$;<br>②将$X$的每一行（代表一个属性字段）进行0均值化；<br>③求出协方差矩阵$S=\cfrac{1}{m}XX^T$ ;<br>④求出协方差矩阵的特征值及对应的特征向量；<br>⑤将特征向量按对应的特征值从上往下从大到小依次排列成矩阵；<br>⑥取前K行组成矩阵$W$；<br>⑦$Z=WX$即为降维到K维后的数据</p><h2 id="4-SVD在PCA中的使用"><a href="#4-SVD在PCA中的使用" class="headerlink" title="4. SVD在PCA中的使用"></a>4. SVD在PCA中的使用</h2><p>奇异值分解(Singular Value Decomposition，SVD)是在机器学习领域广泛应用的算法，它可以用于降维算法中的特征分解。</p><script type="math/tex; mode=display">A_{m\times n}=U_{m\times m}\sum_{m\times n}V_{n\times n}^T=U_{m\times k}\sum_{k\times k}V_{k\times n}^T\tag {10}</script><p>其中k要比n小很多，也就是一个大的矩阵A可以用三个小的矩阵 $U_{m\times k}$、$\sum_{k\times k}$、$V_{k\times n}^T$来表示。如下图所示，现在我们的矩阵A只需要灰色的部分的三个小矩阵就可以近似描述了。</p><p><img src="https://i.loli.net/2021/01/30/H83UKD5vFCARedb.png" alt="奇异值分解"></p><p>PCA降维，需要找到样本协方差矩阵$X_TX$的最大的d个特征向量，然后用这最大的d个特征向量张成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵$X_TX$ ，当样本数多、样本特征数也多的时候，这个计算量是很大的。</p><p>注意到我们的SVD也可以得到协方差矩阵 $X_TX$最大的d个特征向量张成的矩阵，但是SVD有个好处，有一些SVD的实现算法可以不求先求出协方差矩阵$X_TX$，也能求出我们的右奇异矩阵V。也就是说，我们的PCA算法可以不用做特征分解，而是做SVD来完成。这个方法在样本量很大的时候很有效。实际上，scikit-learn的PCA算法的背后真正的实现就是用的SVD，而不是我们认为的暴力特征分解。但SVD的缺点是<strong>分解出的矩阵解释性往往不强</strong>，有点黑盒子的味道，不过这不影响它的使用。</p><h2 id="5-理解PCA的4个境界-To-be-continued"><a href="#5-理解PCA的4个境界-To-be-continued" class="headerlink" title="5. 理解PCA的4个境界 (To be continued)"></a>5. 理解PCA的4个境界 (<strong><em>To be continued</em></strong>)</h2><p>主成分分析PCA算法：为什么去均值以后的高维矩阵乘以其协方差矩阵的特征向量矩阵就是“投影”？ - 史博的回答 - 知乎 <a href="https://www.zhihu.com/question/30094611/answer/275172932">https://www.zhihu.com/question/30094611/answer/275172932</a></p><p>（上文提到的四个境界，暂时只能理解两个，等搞懂另外两个，继续补充。）</p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/343182129">算法理论02 主成分分析PCA - Rorschach的文章 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/74833836">机器学习01—PCA数学推导 - 烟雨小夕的文章 - 知乎</a></li><li><a href="https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ">台大李宏毅主讲的机器学习课程(2016)</a>，课程主页：<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html</a> </li><li><a href="https://zhuanlan.zhihu.com/p/29846048">奇异值分解（SVD） - 漫漫成长的文章 - 知乎</a></li><li><a href="https://www.zhihu.com/question/30094611/answer/275172932">主成分分析PCA算法：为什么去均值以后的高维矩阵乘以其协方差矩阵的特征向量矩阵就是“投影”？ - 史博的回答 - 知乎</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>unsupervised learning</tag>
      
      <tag>dimension reduction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于深度学习的图像分割的loss function</title>
    <link href="/2021/01/24/loss-functon/"/>
    <url>/2021/01/24/loss-functon/</url>
    
    <content type="html"><![CDATA[<h1 id="基于深度学习的自然图像和医学图像分割之”损失函数“"><a href="#基于深度学习的自然图像和医学图像分割之”损失函数“" class="headerlink" title="基于深度学习的自然图像和医学图像分割之”损失函数“"></a>基于深度学习的自然图像和医学图像分割之”损失函数“</h1><p>基于深度学习的自然图像和医学图像分割，包含了网络结构设计、2D序列模型/3D卷积模型设计、损失函数设计、数据增强、融合传统计算机视觉方法、强化学习、可解释性、弱监督学习、自监督学习、预处理模块等方面，本文主要整理”损失函数“相关内容。</p><p>从频率派的角度看深度学习模型，是把输入数据$X$假设为一个随机变量，服从一个概率分布$X$ ~ $p(x|\theta)$，其中参数$\theta$是未知常量。我们需要对$\theta$进行秋节，但深度学习模型直接得到解析解是不可能的，我们只能求的$\tilde{\theta}$来逼近$\theta$。损失函数就是作为优化过程的指导，衡量模型预测结果与真实标签之间的“差距”，然后通过梯度反向传播来不断修正$\tilde{\theta}$使其趋近于$\theta$，这个过程就是优化。这样模型参数的求解问题转化为一个最优化问题。图像语义分割问题可视为一个像素级分类问题，因此最常用的分类损失函数——交叉熵损失函数，可以用于图像语义分割，发展出基于交叉熵的损失函数系列；图像分割结果是一个mask（或概率图），计算预测mask（或概率图）与ground truth的重合度可以直接度量模型的分割性能，基于此发展出基于重合度度量（如IoU）的损失函数系列。下面分别进行详细介绍：</p><h2 id="1-基于交叉熵的损失函数系列"><a href="#1-基于交叉熵的损失函数系列" class="headerlink" title="1. 基于交叉熵的损失函数系列"></a>1. 基于交叉熵的损失函数系列</h2><p>这个系列损失函数基于交叉熵理论进行设计，通过逐像素计算预测分布与ground truth（GT）分布之间的“差距”得到损失函数的值。数学上可证明交叉熵损失函数等价于最大似然估计。</p><h3 id="1-1-交叉熵（Cross-Entropy，CE）"><a href="#1-1-交叉熵（Cross-Entropy，CE）" class="headerlink" title="1.1 交叉熵（Cross Entropy，CE）"></a>1.1 交叉熵（Cross Entropy，CE）</h3><p>交叉熵损失函数逐像素对比了模型预测向量与one-hot编码后的GT，在二类分割问题中，令：</p><script type="math/tex; mode=display">P(Y=1)=p\tag 1</script><script type="math/tex; mode=display">P(Y=0)=1-p\tag 2</script><p>其中，$Y = 1,0$分别表示标签为阳性和阴性。在分割问题中，ground truth是已知的，即$p = 0,1$。</p><p>模型的概率预测结果可以由sigmoid函数（或softmax）计算得到，令：</p><script type="math/tex; mode=display">P(\hat{Y}=1)=\cfrac {1}{1+e^x}=\hat{p}\tag 3</script><script type="math/tex; mode=display">P(\hat{Y}=0)=1-\cfrac {1}{1+e^x}=1-\hat{p}\tag 4</script><p>其中，$x$是模型的输出，后接sigmoid函数可以将其转为概率结果（即各类预测概率之和为1），$\hat{Y} = 1,0$分别表示预测为阳性和阴性。</p><p>那么二分类交叉熵损失函数可以定义为：</p><script type="math/tex; mode=display">CE(p,\hat{p})=-(p\log{\hat{p}}+(1-p)\log{(1-\hat{p})})\tag 5</script><p>推广即可得到多分类分割的交叉熵损失函数公式：</p><script type="math/tex; mode=display">CE=\sum_{class}p\log \hat{p}\tag 6</script><blockquote><p>这里要说明一下，在从二分类推广到多分类分割问题时，需要用到one-hot编码。这在语义分割任务中是一个必不可少的步骤。一般情况下，我们分割的目标是为输入图像的每个像素预测一个标签：</p><p><img src="https://i.loli.net/2021/01/24/rQPnogTz4IN1maG.jpg" alt="语义分割原图与分割结果对比">但是FCN类网络输出结果是h*w*C的特征图，想要在特征图与GT之间 计算Loss值 ，就必须进行转换使两者额的shape对应，而且每个像素点拥有对每一类的预测概率。因此，对于网络输出的特征图（假设预定类别数为C），我们使网络输出特征图为h*w*C然后对每个像素位置的所有通道进行softmax操作，以使其表示为预测概率，最终通过取每个像素点在所有 channel 的 argmax 可以得到该像素点最终的预测类别。<br>对于数据标签（mask），为每一个类别创建一个输出通道（one-hot编码）。</p><p><img src="https://i.loli.net/2021/01/24/g1Sf579T28XldRD.jpg" alt="GT经过one-hot编码后"></p></blockquote><h3 id="1-2-加权交叉熵（Weighted-Cross-Entropy，WCE）"><a href="#1-2-加权交叉熵（Weighted-Cross-Entropy，WCE）" class="headerlink" title="1.2 加权交叉熵（Weighted Cross Entropy，WCE）"></a>1.2 加权交叉熵（Weighted Cross Entropy，WCE）</h3><p>交叉熵损失分别计算每个像素的交叉熵，然后对所有像素进行平均，这意味着我们默认每类像素对损失的贡献相等。 如果各类像素在图像中的数量不平衡，则可能出现问题，因为数量最多的类别会对损失函数影响最大，从而主导训练过程。 Long等提出了为每个类加权的交叉熵损失（WCE），以抵消数据集中存在的类不平衡。以二类分割为例，WCE可被定义为：</p><script type="math/tex; mode=display">WCE(p,\hat{p})=-(\beta p\log{\hat{p}}+(1-p)\log{(1-\hat{p})})\tag 7</script><p>当 $\beta &gt;1$ 时，可降低False Negative(FN)比例，当 $\beta &lt;1$ 时，可降低False Positive(FP)比例。即想要减小哪一类的误分率，就给哪一类赋予更大的相对权值。为了同时调整负样本的权值，可以使用BCE(Balanced Cross Entropy)损失函数：</p><script type="math/tex; mode=display">BCE(p,\hat{p})=-(\beta p\log{\hat{p}}+(1-\beta)(1-p)\log{(1-\hat{p})})\tag 8</script><p>Ronnenberger等人在交叉熵函数中添加了一个距离学习距离，加强模型对类间距离的学习，以在彼此之间非常接近的情况下实现更好的分割，公式如下：</p><script type="math/tex; mode=display">BCE(p,\hat{p})+\omega_0\cdot\exp (-\cfrac{(d_1(x)+d_2(x))^2}{2\sigma^2})\tag 9</script><p>其中$d_1(x)$、$d_2(x)$是是两个距离函数，在细胞分割问题中，Lin等用于计算当前像素到最近的和第二近细胞的边界的距离。这个损失函数是在著名的U-Net论文中提出来的。</p><p>小结：对交叉熵损失函数进行加权后，可以削弱样本类数量不平衡引起的问题</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>基于深度学习的自然图像和医学图像分割：损失函数设计(1) - 李慕清的文章 - 知乎 <a href="https://zhuanlan.zhihu.com/p/106005484">https://zhuanlan.zhihu.com/p/106005484</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>image segmentation</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>loss function</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Entropy</title>
    <link href="/2021/01/24/Entropy/"/>
    <url>/2021/01/24/Entropy/</url>
    
    <content type="html"><![CDATA[<h1 id="关于“熵Entropy”的整理"><a href="#关于“熵Entropy”的整理" class="headerlink" title="关于“熵Entropy”的整理"></a>关于“熵Entropy”的整理</h1><p>1948年，香农（Claude Shannon）在他著名的论文“通信的数学原理”（<em> A Mathematic Theory of Communication</em>) 中提出了“信息熵”的概念。信息的信息量与其不确定性有着直接的关系。在通俗的介绍中，熵一般有两种解释：<strong>（1）熵是不确定性的度量；（2）熵是信息的度量。</strong>看上去说的不是一回事，其实它们说的就是同一个意思。首先，熵是不确定性的度量，它衡量着我们对某个事物的<strong>“无知程度”</strong>。熵为什么又是信息的度量呢？既然熵代表了我们对事物的无知，那么当我们从“无知”到“完全认识”这个过程中，就会获得一定的信息量，我们开始越无知，那么到达“完全认识”时，获得的信息量就越大，因此，作为不确定性的度量的熵，也可以看作是信息的度量，说准确点，是我们能从中获得的最大的信息量。</p><h2 id="1-信息熵"><a href="#1-信息熵" class="headerlink" title="1. 信息熵"></a>1. 信息熵</h2><p>香农用“比特”（Bit）这个概念来度量信息量。信息量的比特数和所有可能情况的对数函数$\log$ 有关。熵的基本计算公式为：</p><script type="math/tex; mode=display">H(X)=-\sum_{x \in X}p(x)\log p(x)\tag 1</script><p>这里的$\log$可以是自然对数，也可以是以2为底的对数，事实上，任意大于1的底都是成立的，因为换底只不过相当于换了信息的单位。$p(x)$是量$X$取值为$x$的概率。对于连续的概率分布，熵类似地定义为（把求和换成积分）</p><script type="math/tex; mode=display">H(X)=-\int_{x \in X} p(x)\log p(x)dx\tag 2</script><p>$p(x)$是$X$的概率密度函数</p><h2 id="2-信息的相关性与”条件熵”（Conditional-Entropy）"><a href="#2-信息的相关性与”条件熵”（Conditional-Entropy）" class="headerlink" title="2. 信息的相关性与”条件熵”（Conditional Entropy）"></a>2. 信息的相关性与”条件熵”（Conditional Entropy）</h2><p>信息和消除不确定性是相联系的，不确定性假定为$U$，从外部消除这个不确定性唯一的办法是引入信息$I$，而需要引入的信息量取决于这个不确定性的大小，即$I &gt; U $才行。当$I &lt; U $时，这些信息可以消除一部分不确定性，也就是说新的不确定性。</p><script type="math/tex; mode=display">U^\prime = U - I\tag 3</script><p>如果没有信息，任何公式或者数字的游戏都无法排除不确定性。</p><p>在数学上可以严格的证明为什么这些”相关的”信息也能够消除不确定性。为此，需要引入一个条件熵的概念。</p><p>假定$X$和$Y$是两个随机变量，假定我们现在知道了$X$的随机分布$p(X)$,那么也就知道了$X$的熵，如式（1）所示的信息熵公式。现在假定我们还知道$Y$的一些情况，包括它和$X$一起出现的概率，在数学上称为联合概率分布（JointProbability），以及在$Y$取不同值的前提下$X$的概率分布，在数学上称为条件概率分布（Conditional Probability）。定义在$Y$的条件下的条件熵为：</p><script type="math/tex; mode=display">H(X|Y)= -\sum_ {x\in X,y\in Y}p(x,y)\log p(x|y)\tag 4</script><p>$H(x) \geqq H(X|Y)$, 也就是说多了$Y$ 的信息之后，关于X的不确定性下降了。在数学上证明了二元模型的不确定性小于一元模型。同理可以定义有两个条件的条件熵</p><script type="math/tex; mode=display">H(X|Y,Z)=-\sum_ {x\in X,y\in Y,z \in Z}p(x,y,z)\log p(x|y,z)\tag 5</script><h2 id="3-互信息（Mutual-information）"><a href="#3-互信息（Mutual-information）" class="headerlink" title="3.  互信息（Mutual information）"></a>3.  互信息（Mutual information）</h2><p>香农在信息论中提出了一个“互信息”的概念作为两个随机事件“相关性”的量化度量。</p><p>假定有两个随机事件$X$和$Y$，它们的互信息定义如下：</p><script type="math/tex; mode=display">I(X:Y)= \sum_{x \in X, y \in Y}p(x,y)\log \cfrac{p(x,y)}{p(x)p(y)}\tag 6</script><p>这个互信息熵是随机事件$X$的不确定性或者说熵$H(x)$，以及在知道随机事件$Y$下的不确定性，或者说条件熵$H(X|Y)$之间的<strong>差异</strong>，即</p><script type="math/tex; mode=display">I(X:Y)=H(X)-H(X|Y)\tag 7</script><p>所以，所谓两个事件相关性的量化度量，就是在了解了其中一个$Y$的前提下，对消除另一个$X$不确定性所提供的信息量。需要讲一下，互信息是一个取值在$0$到$min(H(x)，H(y))$之间的函数，当$X$和$Y$完全相关时，它的取值是$H(X)$；当二者完全无关时，它的取值时$0$。</p><h2 id="4-相对熵（Relative-Entropy）"><a href="#4-相对熵（Relative-Entropy）" class="headerlink" title="4. 相对熵（Relative Entropy）"></a>4. 相对熵（Relative Entropy）</h2><p>”相对熵“，在有些文献中也被称为”交叉熵“，相对熵用来衡量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相对性，它的定义如下：</p><script type="math/tex; mode=display">KL(f(x)||g(x))= \sum_{x \in X}f(x)\cdot \log \cfrac{f(x)}{g(x)}\tag 8</script><p>KL代表KL散度，有如下三条结论：</p><ol><li>对于两个完全相同的函数，他们的相对熵等于零。</li><li>相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。</li><li><strong>对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。</strong></li></ol><p>需要指出的是相对熵是不对称的，即</p><script type="math/tex; mode=display">KL(f(x)||g(x)) \ne KL(g(x)||f(x))\tag 9</script><p>为了让它对称，詹森和香农提出一种新的相对熵的计算方法，将上面的不等式两边取平均，即</p><script type="math/tex; mode=display">JS(f(x)||g(x))= \cfrac{1}{2}[KL(f(x)||g(x))+KL(g(x)||f(x))]\tag {10}</script><p>JS表示JS散度。</p><h3 id="交叉熵Cross-Entropy"><a href="#交叉熵Cross-Entropy" class="headerlink" title="交叉熵Cross-Entropy"></a>交叉熵Cross-Entropy</h3><p>在图像分割中，相关的损失函数中包含了基于分布的二元交叉熵（Binary Cross-Entropy）、加权交叉熵（Weighted Cross-Entropy）和平衡交叉熵（Balanced Cross-Entropy）。具体可参考<a href="http://tiamoqin.me/2021/01/24/loss-functon/">基于深度学习的自然图像和医学图像分割之”损失函数“</a>。</p><p><strong>二元交叉熵</strong></p><p>交叉熵的损失函数单独评估每个像素矢量的类预测，然后对所有像素求平均值，所以我们可以认为图像中的像素被平等的学习了。但是，医学图像中常出现类别不均衡 （class imbalance）的问题，由此导致训练会被像素较多的类主导，对于较小的物体很难学习到其特征，从而降低网络的有效性。</p><p>最好的使用场景是不同的类别间有均匀分布的数据，是基于伯努利分布的损失函数。</p><p><strong>加权二元交叉熵</strong></p><p>加权二元交叉熵（WCE）是二元交叉熵变量的一个变种。在这个例子中，正的例子被一些系数加权。它广泛用于倾斜数据的情况。</p><p>加入的权值用于调整假阴性和假阳性。如果你想减少假阴性的数量就设置权值大于1，同样地，如果想要减少假阳性的数量就设置权值小于1.权值是用于正例的系数。</p><p>应用场景：广泛应用于偏斜数据集，通过系数来衡量。</p><p><strong>平衡交叉熵</strong></p><p>平衡交叉熵（BCE）类似于加权交叉熵。唯一的区别是，除了正例子，我们还对负示例进行了加权。</p><p>应用场景：与加权交叉熵类似，广泛应用于倾斜数据集，它分别对正样本和负样本进行加权。</p><h2 id="5-最大熵原理"><a href="#5-最大熵原理" class="headerlink" title="5. 最大熵原理"></a>5. 最大熵原理</h2><p>人们常用“不要把鸡蛋放在同一个篮子里”——来通俗地解释这个最大熵原理（The Maximum Entropy Principle）。“最大熵”说白了，就是要保留全部的不确定性，将风险降到最小。最大熵原理指出，对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。（不做主观假设这点很重要。）在这种情况下，概率分布最均匀，预测的风险最小。因为这是概率分布的信息熵最大，所以人们把这种模型叫做“最大熵模型”。匈牙利著名数学家希萨（I. Csiszar)证明，对任何一组不自相矛盾的信息，这个最大熵模型不仅存在，而且是唯一的。此外，他们都有同一个非常简单的形式——指数函数。</p><h3 id="估算概率"><a href="#估算概率" class="headerlink" title="估算概率"></a>估算概率</h3><p>最大熵原理说明为什么我们认为抛硬币时，正负两面的概率都是$\cfrac {1}{2}$。假设正面的概率为$p$，则熵为</p><script type="math/tex; mode=display">H(p)=-p \log p-(1-p)\log (1-p)\tag {11}</script><p>求导可以算的，当$p=\cfrac {1}{2}$时，$H(p)$最大，如下图所示。</p><p><img src="https://i.loli.net/2021/01/26/74utMy3QwLkZdxI.png" alt="最大熵原理"></p><p>以上都是假设我们是完全无知的情况下作出的估计。事实上，很多时候我们并没有那么无知，我们可以根据大量的统计，得出一些信息，来完善我们的认识。下面介绍根据统计得出的估计框架。</p><h3 id="估计框架"><a href="#估计框架" class="headerlink" title="估计框架"></a>估计框架</h3><h4 id="离散型概率"><a href="#离散型概率" class="headerlink" title="离散型概率"></a>离散型概率</h4><p>要求信息熵$-\sum_{x \in X}p(x)\log p(x)$的最大值。参考<a href="https://spaces.ac.cn/archives/3552">苏剑林</a>的例子，利用微积分中的<strong>带有约束的极值问题</strong>，方法就是<strong>拉格朗日乘子法</strong>，即引入参数$\lambda$，那么原问题等价于式子的极值，得出</p><script type="math/tex; mode=display">p(x)=\cfrac{1}{Z}\exp (-\sum_{i=1}^k \lambda_if_i(x))\tag {12}</script><p>其中$Z$是归一化因子，即</p><script type="math/tex; mode=display">Z=\sum_x \exp(-\sum_{i=1}^k\lambda_if_i(x))\tag {13}</script><p>类似的方法，连续型概率获得如式（12）、（13）一样的式子。</p><p><em>To be continued</em></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>苏剑林. (Dec. 01, 2015). 《“熵”不起：从熵、最大熵原理到最大熵模型（一） 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/3534">https://spaces.ac.cn/archives/3534</a></li><li>苏剑林. (Dec. 11, 2015). 《“熵”不起：从熵、最大熵原理到最大熵模型（二） 》[Blog post]. Retrieved from <a href="https://spaces.ac.cn/archives/3552">https://spaces.ac.cn/archives/3552</a></li><li>吴军《数学之美》</li></ol>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
      <category>basic theory</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>entropy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Radiomics literature study</title>
    <link href="/2020/12/17/radiomics-lit-study/"/>
    <url>/2020/12/17/radiomics-lit-study/</url>
    
    <content type="html"><![CDATA[<h1 id="文献学习：从手工放射组学特征到深度学习分割的放射组学特征"><a href="#文献学习：从手工放射组学特征到深度学习分割的放射组学特征" class="headerlink" title="文献学习：从手工放射组学特征到深度学习分割的放射组学特征"></a>文献学习：从手工放射组学特征到深度学习分割的放射组学特征</h1><p>Afshar P, Mohammadi A, Plataniotis KN, Oikonomou A, Benali H. From handcrafted to deep-learning-based cancer radiomics: challenges and opportunities. IEEE Signal Processing Magazine. 2019 Jun 26;36(4):132-60.</p><p>原文：<a href="https://ieeexplore.ieee.org/abstract/document/8746872/">From Handcrafted to Deep-Learning-Based Cancer Radiomics: Challenges and Opportunities</a></p><p>中文翻译：<a href="https://mp.weixin.qq.com/s?__biz=MzU0NTc5NDM1NQ==&amp;mid=2247493499&amp;idx=1&amp;sn=4a5d2e2d7687c98b35e7c437a4c9ff41&amp;chksm=fb65d727cc125e317d221a7c17a3a90d8474829bcd96fed94f4fe1f47dd77d92c04a4f0841a7&amp;mpshare=1&amp;scene=23&amp;srcid=1129q1p6fnZzOFfo9R9FKiAC&amp;sharer_sharetime=1606645394672&amp;sharer_shareid=06f9b8344dd8522bc9c5694451753da0#rd">从手工放射组学特征到深度学习分割的放射组学特征</a></p><h2 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h2><p><strong>&#8195;&#8195;放射组学是指从医学图像中提取和分析几种半定量(例如，衰减、形状、大小和位置)和或定量特征(例如，小波分解、直方图和灰度强度)的过程，最终目标是获得预测或预测模型。</strong></p><p>Radiomics refers to the process of extracting and analyzing several semiquantitative (e.g., attenuation, shape, size, and location) and/or quantitative features (e.g., wavelet decomposition, histogram, and gray-level intensity) from medical images, with the ultimate goal being the acquisition of predictive or prognostic models.</p><p>&#8195;&#8195;计算机辅助设计(CAD)并不是一个新概念，研究人员以前曾开发了自动系统来研究基于成像的特征与生物特征之间的联系。然而，自2010年以来，该领域被称为放射组学，其与传统的CAD系统相比，有如下几点差异：</p><ol><li><p>CAD系统包含的特征数量要少得多(通常只包含8到20个特征)，而在放射组学中，可从影像中提取数百到数千个特征。</p></li><li><p>CAD系统的应用通常仅限于疾病的诊断，如区分良性和恶性肿块。然而，放射组学是一个更广泛的领域，包括预测和预后应用。</p></li></ol><p>&#8195;&#8195;<strong>放射组学中的关键基本假设是构建的描述性模型（基于医学成像数据，有时由生物学和/或医学数据补充）能够提供相关且有益的预测，预后和/或诊断信息。在这方面，可以确定两大类放射组学。</strong></p><ol><li><strong>基于手工放射组学（handcrafted radiomic, HCR）</strong>特征的传统方法，该方法包括以下四个主要处理任务：1）图像采集/重建，2）图像分割，3）特征提取和量化，4）统计分析和模型构建。</li><li><strong>基于深度学习放射组学（deep-learning-based radiomic, DLR）</strong>方法是最近出现的并且与前一类不同，因为深度网络不一定需要分割ROI，并且它们的特征提取和分析部分是部分或完全耦合的。</li></ol><p>因此，本文将涵盖以下四个主要方面。</p><ol><li><strong>HCR：</strong>在此，我们介绍并研究了<strong>在放射组学背景下使用的不同特征提取、特征降维和分类方法。上述任何步骤中采用的大多数技术都属于机器学习范畴，其目的是利用过去的信息(数据)提高不同计算模型的性能。</strong>换言之，底层模型能够从过去的数据中学习，从而实现预测和诊断的自动化过程。此外，由于提取了数百个放射组学特征，因此必须采用适当的特征选择/提取策略，以减少“维数灾难”和预测模型的过拟合。这些策略本身大多属于机器学习领域，因为它们都旨在基于可用数据学习最佳的特征集。</li><li><strong>DLR</strong>:在此，我们提供了一个在放射组学中使用的不同深度体系结构的概述，以及它们的<strong>可解释性要求</strong>。</li><li><strong>混合解决方案：</strong>这些方案结合了HCR和DLR的优势。</li><li><strong>挑战、未解决的问题及机遇</strong>:在此，我们将重点介绍放射组学独有处理技术的局限性，并向信号处理（signal processing，SP）研究人员介绍未解决的问题和潜在的机遇。</li></ol><h2 id="2-Application"><a href="#2-Application" class="headerlink" title="2. Application"></a>2. Application</h2><p>&#8195;&#8195;近年来，放射组学已应用于许多医疗保健应用，包括肿瘤学（oncology）、心脏病学（cardiology）和神经病学（neurology）。在心脏病学中，放射组学也用于不同的研究，如鉴定冠状动脉斑。在神经病学中，它广泛适用于检测阿尔茨海默病和帕金森病。然而，在放射组学的所有应用中，癌症相关主题一直是主要焦点。在下一节中，我们将讨论几种与癌症相关的应用，这些应用证明了放射组学在此领域的成功。</p><h3 id="2-1-癌症诊断-Cancer-diagnosis"><a href="#2-1-癌症诊断-Cancer-diagnosis" class="headerlink" title="2.1 癌症诊断 Cancer diagnosis"></a>2.1 癌症诊断 Cancer diagnosis</h3><p>&#8195;&#8195;确认患者是否患有癌症是最关键也是最敏感的决策之一，须尽可能快的确诊。放射组学有可能在癌症早期诊断方面提高其准确率。</p><h3 id="2-2-肿瘤检测-Tumor-detection"><a href="#2-2-肿瘤检测-Tumor-detection" class="headerlink" title="2.2 肿瘤检测 Tumor detection"></a>2.2 肿瘤检测 Tumor detection</h3><p>&#8195;&#8195;识别恶性病变对于指导针对性的局部治疗至关重要。</p><h3 id="2-3-肿瘤分类和属性评级-Tumor-classification-and-attribute-scoring"><a href="#2-3-肿瘤分类和属性评级-Tumor-classification-and-attribute-scoring" class="headerlink" title="2.3 肿瘤分类和属性评级 Tumor classification and attribute scoring"></a>2.3 肿瘤分类和属性评级 Tumor classification and attribute scoring</h3><p>&#8195;&#8195;肿瘤分类是指确定肿瘤的类型。通常，癌症分为以下几大类:良性(benigh)、原发性恶性(primary malignant)和转移性恶性(matastatic malignant)。因为肿瘤与不同的属性相关联，例如它们的边界和球形度，分析这些属性有助于更好地理解肿瘤的形状和行为。</p><h3 id="2-4-生存期预测-Survival-prediction"><a href="#2-4-生存期预测-Survival-prediction" class="headerlink" title="2.4 生存期预测 Survival prediction"></a>2.4 生存期预测 Survival prediction</h3><p>&#8195;&#8195;根据图像特性和肿瘤的异质性，放射组学提供了有关患者存活的重要信息。</p><h3 id="2-5-恶性预测-Malignancy-prediction"><a href="#2-5-恶性预测-Malignancy-prediction" class="headerlink" title="2.5 恶性预测 Malignancy prediction"></a>2.5 恶性预测 Malignancy prediction</h3><p>&#8195;&#8195;基于某些因素可以区分肿瘤的良恶性，例如它们是否扩散到其他组织。良性肿瘤通常不会扩散到其他器官，但可能需要手术切除，因为它们可能会变大。恶性肿瘤会危及生命，也可能会扩散到其他器官，由此需要如化疗般更为复杂的治疗方式。因此，用放射组学这种非侵入性方法进行预测肿瘤恶性预测至关重要。</p><h3 id="2-6-复发预测-Recurrence-prediction"><a href="#2-6-复发预测-Recurrence-prediction" class="headerlink" title="2.6 复发预测 Recurrence prediction"></a>2.6 复发预测 Recurrence prediction</h3><p>&#8195;&#8195;因为癌变区域需要切除或治疗，所以没有强有力的标志或证据来帮助预测复发。然而，最近已采用放射组学来帮助解决此类问题，并显示出有前景的初步结果。</p><h3 id="2-7-癌症分期-Cancer-staging"><a href="#2-7-癌症分期-Cancer-staging" class="headerlink" title="2.7 癌症分期 Cancer staging"></a>2.7 癌症分期 Cancer staging</h3><p>&#8195;&#8195;癌症可能在不同阶段被诊断出来。例如，它们可能处于早期阶段，这意味着它们保留在它们首次出现的同一组织中，或者它们可以处于晚期阶段，这意味着它们已经扩散到其他组织。了解肿瘤的阶段对治疗的选择有重大影响。</p><p>&#8195;&#8195;基于上述类别，表1总结了在各种文章中引入的放射学的不同应用领域，以及它们相关的放射组学方法（即HCR，DLR或两者的组合）。这些表格还提供了与放射组学结合使用的任何补充数据源的信息。</p><p><img src="https://i.loli.net/2020/12/19/YaGwPmbdCOxni6H.png" alt="表1 放射组学的应用"></p><h2 id="3-HCR的最新技术"><a href="#3-HCR的最新技术" class="headerlink" title="3. HCR的最新技术"></a>3. HCR的最新技术</h2><p>&#8195;&#8195;HCR特征的研究通常包括以下关键步骤:</p><ol><li><strong>预处理(Preprocessing):</strong>预处理是为了减少原始数据中的噪声和伪像而引入的，通常包括<strong>图像平滑(image-smoothing)和图像增强(image-enhancement)技术.</strong></li><li><strong>分割(Segmantation):</strong>分割是HCR工作流程中的关键步骤，因为需要从分割的区域中提取HCR特征，而许多病灶并没有明显的边界。<strong>尽管手动勾画大体肿瘤边界是常规(标准)的临床方法，但其不仅耗时而且对不同勾画者的可变性极其敏感。因此需要开发高精度的高级(半自动)分割解决方案，该解决方案还可生成可再现的肿瘤边界。</strong></li></ol><p>&#8195;&#8195;自动和半自动分割技术可以是常规的方法(即使用预定义的特征将图像像素/体素分类为肿瘤或非肿瘤)，也可以是基于深度学习的(即使用深度网络对图像进行分割)新兴技术。传统技术本身可以分为三类:<strong>基于强度、基于模型和机器学习方法</strong>。在前一类中，强度被用作像素的主要区别特征。而在基于模型的方法中，目的是通过优化能量函数来改善初始轮廓。然而在机器学习方法中，需要从像素中提取一组特征，包括亮度和梯度，然后将这些特征用作机器学习模型(例如支持向量机(SVM))的输入，以对像素进行分类。然而，<strong>常规技术存在几个缺点。例如，肿瘤的密度有时可能与其他组织相似；因此，强度可能不是良好的鉴别器。此外，在基于模型的分割中，能量函数的公式化可能涉及大量参数，这使得能量函数的优化困难且耗时。</strong>另一方面，深度学习方法能够学习最能区分肿瘤像素和非肿瘤像素的特征，并且可以以端对端的方式进行学习。深度学习方法，如U-Net的不同变体、“LungNet”架构、DenseNet和混合扩张卷积，目前更常用于医学图像分割。</p><p>放射组学工作流程由五个步骤组成，前两个步骤是预处理和分割。其余三个步骤则是:</p><ul><li><strong>特征提取(Feature extraction)</strong>：特征提取是放射组学工作流程中的主要步骤，将在“基于强度的特征”一节中进行讨论。</li><li><strong>特征降维(Feature reduction)</strong>：是放射组学中的另一个关键步骤，因为尽管可以从可用的大图像数据集提取大量定量特征，但大多数特征之间可能高度相关，也可能与当前任务无关，和/或促进模型的过拟合。为了解决这些问题，在“基于形状的要素”一节中讨论了放射组学的特征减少技术。</li><li><strong>统计分析(Statixtical analysis)</strong>：统计分析是指在“应用”部分所述的特定应用中利用所提取的放射组学特征。我们将在“基于纹理的特征”一节中进一步阐述此类基于放射组学的统计分析。</li></ul><h3 id="3-1-放射组学特征提取"><a href="#3-1-放射组学特征提取" class="headerlink" title="3.1 放射组学特征提取"></a>3.1 放射组学特征提取</h3><p>&#8195;&#8195;放射组学进行特征提取时，可以提取不同类型的特征，通常可分为三类:<strong>（1）一阶特征(基于强度和基于形状的特征)、（2）二阶特征(基于纹理的)和（3）高阶特征。</strong>表2进行了不同的潜在特征的总结。但请注意，HCR特征不限于此列表，实际中可提取超过数百个特征(如[5]中，在进行特征降维之前，提取了400个HCR初始特征)。接下来，我们进一步研究最常用的HCR特征类别。</p><p><img src="https://i.loli.net/2020/12/19/hPfeaDdSLjowt1X.png" alt="表2 放射组学中常用的不同类型的HCR特征"></p><h4 id="3-1-1-基于强度的特征"><a href="#3-1-1-基于强度的特征" class="headerlink" title="3.1.1 基于强度的特征"></a>3.1.1 基于强度的特征</h4><p>&#8195;&#8195;基于强度的方法将多维感兴趣区域转换为单个直方图(描述像素强度的分布)，并从中导出简单的基本特征(例如，<strong>能量energy、熵entropy、峰度kurtosis和偏斜度skewness</strong>)。通过强度特征，我们可以研究肿瘤强度直方图的特性，例如<strong>锐度sharpness、离散度dispersion和不对称度asymmetry。</strong>但是，这些特征对切片厚度等图像采集参数最敏感。因此，设计基于强度的要素时需要特别小心，也要重视数据预处理过程。<strong>在所有的强度特征中，熵和均匀性出现频率最高。一般而言，熵度量像素强度内的随机性程度，并且在所有强度以相等的概率(完全随机性)出现时取其最大值。均匀性则是评估像素强度的一致性，并在所有像素值相同时取其最大值。</strong></p><p>&#8195;&#8195;尽管基于强度的特征计算简单，并且有可能将良性和恶性肿瘤等几种组织与其他组织区分开来，但也有一些缺点。首先，所选的直方图bins数量会对这些特征产生很大影响，<strong>因为太小或太大的bin可能无法正确拟合于底层分布，</strong>因此，这些特征并不总是可靠。此外，优化直方图bin的数量也可能存在问题，因为这会导致不同ROI的bin数量不同，也很难比较各种研究的结果。</p><h4 id="3-1-2-基于形状的特征"><a href="#3-1-2-基于形状的特征" class="headerlink" title="3.1.2 基于形状的特征"></a>3.1.2 基于形状的特征</h4><p>&#8195;&#8195;虽然放射学家通常使用形状特征(也称为语义或形态特征)，但放射组学的目的是在计算机辅助下对其进行量化。这些特征提取自肿瘤区域的2D或3D结构，以定量研究肿瘤的不同形状和大小。</p><p>&#8195;&#8195;在各种基于形状的要素中，<strong>体积volume、表面 surface、球形度sphericity、紧密度compactness、直径diameter和平坦度flatness</strong>,在放射组学中更为常用。例如，球形度可测量体积或感兴趣区域的圆度，特别适用于预测肿瘤的恶性程度，因为良性肿瘤通常比恶性肿瘤更像球体。紧密度是根据球形度定义的，因此无需同时计算这两个值；它们中的一个很可能被以特征冗余为目标的特征选择方法排除。</p><h4 id="3-1-3-基于纹理的特征"><a href="#3-1-3-基于纹理的特征" class="headerlink" title="3.1.3 基于纹理的特征"></a>3.1.3 基于纹理的特征</h4><p>&#8195;&#8195;基于形状和基于强度的特征无法提供有关给定图像上不同像素之间相关性的有用信息。在这方面，基于纹理的特征是最为实用，特别是对于组织异质性起重要作用的研究，因为基于纹理的特征可以捕获相邻像素之间的空间关系。在放射组学中，通常基于不同的描述性矩阵提取基于纹理的特征。其中，gray-level cooccurrence matrix (GLCM),gray-level run-length matrix(GLRLM)、neighborhood gray-tone difference matrix(NGTDM)和gray-level zone-length matrix(GLZLM)是最常用的</p><ul><li><strong>GLCM模拟像素强度的空间分布，可以通过考虑所有强度值对的出现频率来计算。</strong>从GLCM提取的特征是放射组学中最常用的纹理特征。每个GLCM都与两个预定义参数相关联，其中θ和d是图像维度内允许的任何整数距离：<script type="math/tex; mode=display">\theta \in \{ {0^\omicron,45^\omicron,90^\omicron,和135^\omicron}\}</script></li></ul><ul><li><p><strong>GLRLM定义了具有相同强度值的相邻像素的数量，</strong>例如，GLRLM矩阵的(i，j)元素确定了强度值i在θ方向上与j一起出现的次数。</p></li><li><p><strong>基于图像视觉特征的NGTDM是一个矢量，</strong>其第k个元素被定义为强度值为k的所有像素与其邻域的平均强度之间的差异的总和。</p></li><li><p><strong>GLZLM在矩阵中寻找区域</strong>。区域可以定义为共享相同强度像素/体素的集合。GLZLM的（i，j）表示第i个元素对应于强度为i和尺寸为j的区域的数量。</p></li></ul><h4 id="3-1-4-高阶放射组学特征"><a href="#3-1-4-高阶放射组学特征" class="headerlink" title="3.1.4 高阶放射组学特征"></a>3.1.4 高阶放射组学特征</h4><p>&#8195;&#8195;经小波和傅里变换等方式获得的高阶特征可捕获各种频率的成像生物标志物。小波特征是放射组学中使用频率最高的高阶特征。小波粗系数和细系数分别表示纹理和梯度特征，计算方法是将图像乘以一个矩阵（包括复杂的线性或径向“小波母函数”）。傅里叶特征也可捕获梯度信息。<strong>闵可夫斯基函数(MF)</strong>是另一种常见的高阶特征提取器，它考虑强度高于预定义阈值的像素模式。综上所述，MFs最初是通过利用在最小和最大强度限制内的几个阈值来形成ROI的二值化来计算的。虽然使用的阈值数量是一个自由参数，但为了获得更好的结果，应通过科学的选择机制进行确定(通常使用经验检验)。基于二值化的ROI，不同的MFs如面积和周长可如下计算:</p><script type="math/tex; mode=display">\ MF_{area} = n_s\\\tag1</script><script type="math/tex; mode=display">MF_{perimeter} = -4n_s+2n_e\\\tag2</script><p>其中$n_s$和 $n_e$分别是白色像素(超过阈值)和边缘的总数。</p><p>以上，我们就完成了对放射组学中使用的特征提取方法的介绍。</p><h3 id="3-2-放射组学特征降维技术"><a href="#3-2-放射组学特征降维技术" class="headerlink" title="3.2 放射组学特征降维技术"></a>3.2 放射组学特征降维技术</h3><p>&#8195;&#8195;特征降维是放射组学的另一个关键步骤，因为尽管可以从图像数据集中提取到大量定量特征，但其中大部分特征之间高度相关，其与研究任务无关，和/或导致模型过拟合（即，使其对噪声高度敏感）。特征降维方法可以分为有监督和无监督两类。有监督方法（例如过滤和包装方法）考虑了特征的区分能力，并可以根据预定义类别进行数据区分。相反，包括主成分分析（principal component analysis, PCA）、独立成分分析和方差阈值法(等无监督方法旨在减少冗余特征。</p><p>&#8195;&#8195;总之，在减少放射组学中的特征空间时，可以定义各种目标。本研究为特征选择目的定义了以下关键特征：</p><ol><li>可重复性（Reproducibility）：这些特征（也称为稳定特征stable features）对预处理和手动注释更为稳健。</li><li>信息性和相关性（Informativeness and relevancy）：这些特征是与目标变量高度相关的特征。</li><li>冗余（Redundancy）：非冗余特征彼此之间的相关性很小。特征冗余可理解为先前已经选择过的特征，在后来的分析再次出现。如果特征之间的相关性很强，则存在特征冗余现象。</li></ol><h4 id="3-2-1-有监督的特征选择方法"><a href="#3-2-1-有监督的特征选择方法" class="headerlink" title="3.2.1 有监督的特征选择方法"></a>3.2.1 有监督的特征选择方法</h4><p>特征选择的有监督方法通常分为两类。</p><ol><li><strong>过滤Filtering（单变量univariate）方法：这些方法在考虑要素与类标签之间的关系时，是单个变量进行的，没有考虑特征之间的冗余性。</strong>在所有过滤方法中，基于Wilcoxon检验的方法表现出更高的稳定性。Wilcoxon检验是一种非参数统计假设检验技术，用于确定两个不同特征集间的依赖关系，即它们是否具有相同的概率分布。</li><li><strong>包裹Wrapper（多变量multivariate）方法：过滤方法忽略了特征之间的关系，这影响了包裹式方法的发展。与过滤方法相比，包裹式方法研究不同特征子集组合的预测性能，相关性和冗余度的加权和作为性能评分。</strong></li></ol><p>&#8195;&#8195;包装方法包括逐步加入特征和逐步删除特征两种方法。在逐步加入特征进行特征选择时，特征集以一个空集开始，并分别计算所有特征与其类标签的相关性。因此，会选择相关性最高的要素并将其添加到集中。紧接着，将剩余的特征逐个添加到该集合中，以测试所获得的集合的性能，并持续该过程，直到特征加入仍无法提高模型预测性能为止。逐步删除特征选择法与逐步加入特征选择法完全相反，它从包含所有可用特征的集合开始，逐渐减少特征，直到减少特征也无法提高性能为止。</p><p>&#8195;&#8195;由于有监督方法是基于类标签进行学习训练模型，<strong>因此会出现过拟合情况。</strong>并且基于给定的特征集进行的训练模型，可能在别的研究中无法使用。</p><h4 id="3-2-2-无监督的特征选择方法"><a href="#3-2-2-无监督的特征选择方法" class="headerlink" title="3.2.2 无监督的特征选择方法"></a>3.2.2 无监督的特征选择方法</h4><p>&#8195;&#8195;无监督方法通过移除冗余特征(即相关但不提供任何附加信息的特征)来降低特征空间的维数。虽然这些方法不容易出现过拟合，但并不能保证得到的是最优的特征空间。无监督方法又可以<strong>分为线性方法和非线性方法，</strong>其中线性方法假设特征位于线性空间上。因为在放射组学研究领域，通常使用较为简单和基础的无监督学习算法，如主成分分析，所以本文未涉及太多介绍。但是，这为在SP研究中开发更先进的基于统计的降维解决方案提供了机会。</p><h3 id="3-3-放射组学统计分析"><a href="#3-3-放射组学统计分析" class="headerlink" title="3.3 放射组学统计分析"></a>3.3 放射组学统计分析</h3><p>&#8195;&#8195;统计分析是指利用提取的放射学特征进行癌症诊断、肿瘤分期、生存分析等特定研究。虽然大多数统计方法最开始并没有对特征的重要程度进行区分，并对所有预测因子均使用相同权重，但在放射组学领域，最成功的方法是利用专家的先验信息对特征进行相应假设。[5]和[9]中采用聚类方法对所提取的特征进行放射组学分析，并在聚类结果和临床结果之间寻找关联。例如，属于相同类别的患者可能有相似的诊断结果。观察结果显示，图像生物标志物与肿瘤恶性度等临床结果相关。层次聚类是放射组学中最常用的方法。但是，研究通常没有针对目标预测目的对聚类方法进行训练学习，因此必须使用根据预定义的类别标签训练特定的预测模型。放射组学的预测方法分为以下两种模型：</p><ol><li><strong>分类与回归模型，与其他多媒体领域相似，都是以预测离散型或连续型变量为目的。随机森林(Random forests，RF)、SVM和NNs是最常用的回归和分类技术。</strong></li><li><strong>生存分析，是指与时间相关time-related的模型，被用来预测患者的生存时间。这些模型也可用于测试新疗法的有效性。</strong></li></ol><p>&#8195;&#8195;几种放射组学分析方法。由于属于前一类(分类和回归模型)的预测因子在其他多媒体应用中也很常见，因此本文未详细描述。然而，生存分析在放射组学的应用中更为贴合，因此，我们讨论了该类别中三种最常见的方法：<strong>Kaplan-Meier(KM)生存曲线、Cox比例风险(回归)模型(PHM)和对数秩检验log-rank test。</strong></p><h4 id="3-3-1-KMS曲线"><a href="#3-3-1-KMS曲线" class="headerlink" title="3.3.1 KMS曲线"></a>3.3.1 KMS曲线</h4><p>&#8195;&#8195;KMS曲线表示用于测量给定时间点t下的存活概率S(t)轨迹，即：</p><script type="math/tex; mode=display">S(t)=\frac {Number\;of\;patients\;survived\;until\;t}{Number\;of\;patients\;at\;the\;beginning}\tag3</script><p>可以计算所有放射学特征的KMS曲线，以评估不同特征对患者生存的影响，计算步骤如下：</p><p>1）选择要计算KMS曲线的所需特征。</p><p>2）基于所选择的特征，设定一个或多个阈值，其可以将患者分成不同组别，例如低风险和高风险癌症患者，然后根据患者的相关特征是高于还是低于阈值确定患者所属组别。</p><p>3）计算所有获得组的KMS曲线，结果可用于比较不同状态下的患者的存活率。例如，在文献[5]中，高异质性特征与较短的存活时间相关，而高紧密性特征与较长的存活相关。</p><h4 id="3-3-2-Cox（回归）PHM"><a href="#3-3-2-Cox（回归）PHM" class="headerlink" title="3.3.2 Cox（回归）PHM"></a>3.3.2 Cox（回归）PHM</h4><p>&#8195;&#8195;Cox(回归)PHM通常用于医学领域，该算法根据一个或多个预测因子(称为协变量)，如放射特征，去预测患者的生存期。PHM模型的输出用h(t)表示，代表特定时间t时死亡的风险，可以通过以下公式计算：</p><script type="math/tex; mode=display">h(t)=h_0(t)\times exp^{\sum_{i=1}^{N_c} b_ix_i}\tag4</script><p>其中$x_i$是预测因子(协变量)，$b_i$为预测因子的权重。公式中的指数项就是风险大小，通常被假设为特征的线性组合。</p><p>&#8195;&#8195;然后通过基于历史数据的训练过程来计算风险系数($b_i，for(1≤I≤1)$)。更现实地说，Risk可以是一般的非线性函数，即Risk=f(x)。非线性是通过深度学习架构进行训练学习的，这还没有在放射组学的背景下进行过研究。</p><h4 id="3-3-3-对数秩检验"><a href="#3-3-3-对数秩检验" class="headerlink" title="3.3.3 对数秩检验"></a>3.3.3 对数秩检验</h4><p>&#8195;&#8195;log-rank检验用于比较两个样本的存活率，尤其是当这两个样本接受了不同的治疗时。这种检验是一种非参数假设检验，用于评估两条生存期曲线是否存在显著差异。与对数秩检验相关的一个限制是样本大小会影响检验结果，因此，样本量的扩大仍是一个基本需求。</p><h3 id="3-4-HCR评估"><a href="#3-4-HCR评估" class="headerlink" title="3.4 HCR评估"></a>3.4 HCR评估</h3><p><strong>&#8195;&#8195;总之，要获得成功的HCR研究，需要进行精确设计，以选择最佳的特征提取、特征降维和分析方法组合。</strong>最后，需要注意的是，与其他领域相比，报告准确性在放射组学中的信息度量程度较低。在医学领域，预测错误的概率是不对等的，如对阳性和阴性样本进行分类。因此，在放射组学中，能够区分假阳性(FP)和假阴性(FN)误差的测量方法更受青睐。其中一项指标是接受者工作特性<strong>曲线下的面积（即AUC）</strong>，通过该指标可以研究不同判断阈值对FP和FN的影响。混淆矩阵是另一种常用且有用的评估方法，用于检验放射组学分类器的性能，包括其FFP率和FN率。在实践中，医疗领域的大多数决定都是概率性决策，医生在确定决策阈值时通常会考虑多个因素，如特定判断所带来的坏处和好处。然而，这些因素并未在放射组学中进行量化和利用，这就需要对包含医生常用因素的潜在解决方案进行广泛调查。</p><h3 id="3-5-放射组学的稳定性"><a href="#3-5-放射组学的稳定性" class="headerlink" title="3.5 放射组学的稳定性"></a>3.5 放射组学的稳定性</h3><p>&#8195;&#8195;放射组学的一个重要方面是特征提取的稳定性，它量化了特征与预处理步骤之间的依赖程度。放射组学技术的稳定性通常根据以下两种技术中的任意一种进行评估:</p><ol><li><strong>测试/重测：该方法对患者进行多次影像学检查，并单独采集图像。然后从所有获得的影像集中提取放射组学特征并进行分析。这里，不同图像集的不变性说明了放射组学特征的稳定性。</strong></li><li><strong>观察者间可靠性：</strong>该方法是指一个实验流程，要求多个观察者从同一幅图像中描绘出感兴趣区域，并从所有不同的描绘中提取放射特征，以测试它们在分割变化中的稳定性。这里，不同观察者的分割ROI所提取特征的不变性说明了放射组学特征的稳定性。</li></ol><p>使用不同的稳定性标准来确定放射组学中的稳健特征，现将这些标准简述如下。</p><ol><li><strong>组内相关系数(intraclass correlation coefficient, ICC)：</strong>这是一种用于测量放射组学特征稳定性的方法，简称ICC。该方法均可用于上述两个类别(即测试/重测和观察者间可靠性)。<strong>ICC定义为要素可靠性的度量，取0到1之间的值，其中“0”表示无可靠性，“1”表示完全可靠。</strong>将术语BMS和WMS定义为受试者之间和受试者内部的均方值(方差的度量)，其是根据测试/重测设置的单因素方差分析(ANOVA)计算得出的，ICC可估计为：</li></ol><script type="math/tex; mode=display">ICC_{Test-Retest}= \frac{BMS-WMS}{BMS+(N-1)WMS}\tag5</script><p>其中N为重复检查次数，通过将EMS定义为双向ANOVA的残差均方，将M定义为观测器数量，对于观察者间可靠性，ICC可计算如下:</p><script type="math/tex; mode=display">ICC_{Interobserver}= \frac{BMS-WMS}{BMS+(M-1)WMS}\tag6</script><ol><li><strong>Friedman检验：</strong>Friedman检验尤其适用于评估观不同观察者的稳定性，它是一种非参数重复测量，用于估计多个观测值的分布之间是否存在显著差异，并且不需要服从高斯分布。基于该检验，<strong>最稳定特征的稳定秩为“1”。</strong></li></ol><p>&#8195;&#8195;文献[5]说明了稳定性较高的放射性特征具有更好的预测性能，因此，<strong>稳定性分析也可以理解为一种特征降维技术。</strong>根据文献[9]可知，高斯拉普拉斯、基于强度和纹理的特征对于肺部CT图像更稳定，而小波和基于形状的特征对分割的变化敏感。然而，也有其他影响特征稳定性的变化源(分割步骤除外)，其中之一是图像强度离散化方法，该方法尤其对纹理特征有很大影响。医学图像的离散化主要有两种方式：第一种是对所有图像采用固定的bin大小，第二种是使用固定数量的bin。尽管结果表明两种方法均可获得取决于强度分辨率的纹理特征，但第一种方法(固定大小的bin)可获得更稳定的特征。然而，纹理分析需要标准化强度离散化方法，其才能作为一种有意义和可靠的放射组学技术。</p><p>&#8195;&#8195;最后，值得一提的是，<strong>图像生物标志物标准化倡议是一项国际合作，旨在为放射性组学的可重复性挑战提供定义、指导策略和放射组学步骤。其提供的指南涵盖了放射组学分析流程的几个步骤，从图像采集、图像预处理和图像分割到特征计算。</strong>为了总结我们对HCR的研究结果并详细阐述其应用，我们在“肺癌分析的放射组学”中提供了一个例子，其中研究了基于放射学的肺癌分析的详细问题。</p><h3 id="3-6-放射基因组学在DLR的最新技术"><a href="#3-6-放射基因组学在DLR的最新技术" class="headerlink" title="3.6 放射基因组学在DLR的最新技术"></a>3.6 放射基因组学在DLR的最新技术</h3><p>&#8195;&#8195;放射性组学通常与基因组数据相结合，被称为放射性基因组学。换言之，<strong>放射基因组学是指疾病的影像学特征与其基因表达模式、基因突变等基因组相关特征之间的关系。</strong>成像结果与分子诊断数据之间的潜在关联可作为患者治疗反应的预测因子，并为临床护理环境中的决策任务提供重要支持。因此，放射基因组学有可能在不使用活检等侵入性方法的情况下研究癌症。<strong>使用各种关联的挖掘和聚类方法来确定基因表达与放射组学之间的关系</strong>；例如，在[3]中，发现仅28个放射特征就能够重建人肝癌细胞中78%的全局基因表达。</p><p>&#8195;&#8195;要评估基因表达与良性和恶性肿瘤等离散型数据之间的关联，应首先根据基因的区分能力对其进行排序。然而，放射基因组学的目标是发现基因表达和放射组学特征之间的联系。因此，辨别能力是不可定义的。<strong>Spearman秩相关(Spearman’s rank correlation,SRC)系数可用于测量特定放射特征与基因表达之间的相关性。然后根据基因的SRC系数而不是区分能力对其进行分类。</strong>有序基因通常存储在列表L中，传统方法关注列表L中的顶部和底部基因，他们分别表示具有最强正相关和负相关的基因。然而这种方法受到几个方面的限制（如生物性解释困难），因此又引入了“基因集富集分析”(gene set enrichment analysis,GSEA）。GSEA方法是使用最广泛的放射基因组学方法之一。每个基因集S是一组在先前的生物学知识方面相似的基因（例如参与了共同的生物学途径）。GSEA分析的目的是确定给定基因集S的特征是倾向于出现在列表L的顶部还是底部。在这种情况下，该基因集的表达与特定的放射组学特征相关联。如图2所示，使用GSEA分析的放射性基因组学结果是一张热图，代表了所有基因集和放射性特征之间的关联程度。</p><p><img src="https://i.loli.net/2020/12/21/TphcKMFuEjxreyq.png" alt="图2.采用GSEA方法进行放射基因组学分析。首先根据基因与放射特征的关联对基因进行分类，然后根据有序基因列表中的基因集模式形成最终的热图。"></p><p>&#8195;&#8195;放射基因组学的主要作用是，它允许基于对临床结果和基因组学之间关系的先验知识来利用不完善的数据集（其中难以收集临床结果或需要延长收集期）得出新的结论。例如，在一个研究项目中，可能有影像学数据和基因组相关数据，但没有临床结果。如果事先了解基因组学与某些临床结果的关系，那么通过将影像数据与基因组学相关联，可以得出影像数据与临床结果之间关系的新结论。在这种情况下，基因组学可以填补知识空白。</p><h2 id="4-DLR-深度学习放射组学-的最新技术"><a href="#4-DLR-深度学习放射组学-的最新技术" class="headerlink" title="4 DLR(深度学习放射组学)的最新技术"></a>4 DLR(深度学习放射组学)的最新技术</h2><p><strong>&#8195;&#8195;DLR(有时称为发现放射组学discovery radiomics或放射组学序列发现radiomics sequence discovery，序列指特征)是根据预定义任务(包括但不限于疾病诊断、癌症类型预测和生存期预测)的要求，从医学图像中提取深度特征的过程。</strong>可以通过不同的架构(即，线性和非线性函数的堆栈)，例如CNN或自动编码器提取DLR，以从输入中找到最相关的特征（图3所示为深度特征提取示意图），然后，提取的特征可以通过深层网的其余部分进行分析和决策。提取的深度特征也可以通过不同的分类器进行相关分析，例如SVM或决策树(DT)。</p><p><img src="https://i.loli.net/2020/12/21/Zl9nfXJxpAKkECz.png" alt="图3 深度特征提取示意图：网络的输入可以是原始图像、分割的感兴趣区域或两者的组合。提取的放射组学特征将用于网络的其余部分，或者使用外部模型根据放射组学特征作出决策。"></p><h3 id="4-1-DLR相对于HCR的优势"><a href="#4-1-DLR相对于HCR的优势" class="headerlink" title="4.1 DLR相对于HCR的优势"></a>4.1 DLR相对于HCR的优势</h3><p>&#8195;&#8195;使用DLR的一个<strong>主要好处是：无需对感兴趣区域进行精准分割，</strong>将原始图像输入深度学习网络便可提取放射组学特征，这一优势从以下两个方面进行解释。</p><ol><li><p>DLR将手动勾画ROI的负担从专家和放射科医生身上解除，去除分割步骤可以大大减少算法计算时间和成本。此外，<strong>手动标注的数据高度依赖于观察者，因此提供的组学特征稳定性较差。</strong></p></li><li><p>采用自动分割算法对ROI进行分割，算法精度有限，导致分割过程容易出错，并且分割精度堪忧，以至于无法用于敏感的决策过程。<strong>此外，深度学习网络的输入可以是原始图像、分割图像，或是预处理后的图像，亦或是他们的任何组合。对于输入的图像，通过三维连接方式进行匹配。</strong>各种图像的输入类型可以是不同角度的图像，如冠状位和横轴位图像。</p></li></ol><p><strong>一般来说，可以从以下几个方面对DLR的研究进行分类。</strong></p><ol><li><p><strong>输入层次：</strong>深网的输入可以是与特定患者相关的单个切片、整个体积、甚至是整个影像检查图像。每种情况都需要自己的处理方法，例如，当处理输入的整个体积时，应考虑不同患者不同层数的处理。递归神经网络(RNN)可以允许不同输入，我们将在“放射组学的深度学习架构”一节中对其进行简要讨论。</p></li><li><p><strong>预训练模型和原始模型：</strong>根据可用数据集的大小以及研究者时间规划安排，可以对预训练模型进行微调，也可以从头开始训练原始模型。这将在“预处理或原始模型”一节中进行更全面的分析。</p></li><li><p><strong>深度学习网络体系结构：</strong>选择深度网络是提取有意义和实用的DLR时最重要的决定，这将在“放射组学中的深度学习网络体系结构”一节中讨论。</p></li></ol><p>&#8195;&#8195;在本节的后续部分，我们将从<strong>输入层次结构、预处理模型与原始模型以及深度学习网络架构</strong>等不同角度回顾深度放射组学的现状。图4显示了不同DLR方法的分类，为本节的其余部分提供了指导。</p><p><img src="https://i.loli.net/2020/12/21/pzEM95FKo64QtUD.png" alt="图4 DLR的分类。CAEs:卷积自动编码器；DAEs:降噪自动编码器；DBNs:深度信念网络；DBMs：深玻耳兹曼机"></p><h3 id="4-2-输入层结构"><a href="#4-2-输入层结构" class="headerlink" title="4.2 输入层结构"></a>4.2 输入层结构</h3><p>&#8195;&#8195;如图5所示，DLR研究的输入图像可分为三个主要类别：切片级别、体积级别和患者级别。切片级分类是指对图像切片进行相互独立的分析和分类。但是，这种方法的信息量太少，因为我们通常需要基于整个感兴趣区域体积（3D）所对应的标签来作出决定。</p><p>&#8195;&#8195;切片级分类的缺点让体积级分类的方法得以发展，<strong>体积级分类是指通过投票系统融合切片级输出的结果，或者将与体积相关联的所有图像切片用作分类器的输入。</strong>最后，患者级分类是指根据一系列研究(如多次CT随访成像)为患者分配标签。例如，文献[58]中根据一系列CT影像进行恶性肺肿瘤概率预测，实现患者级分类探索。为了达到这一目标，最初，研究者训练一个简单的三层CNN模型来从与单个CT序列（体积级分类）相关的肿瘤斑块中获得预测结果，其目的是将预测的恶性肿瘤率与实际发生率之间的差异最小化。然后，采用一个事先训练过的CNN，计算属于病人的所有序列的恶性率，并通过选择最大恶性率来做出最终决定。换言之，如果至少有一个预测的比率高于预定的恶性肿瘤率，则患者被诊断为恶性肺癌。</p><p><img src="https://i.loli.net/2020/12/21/UVCeo4pr3nqDMJh.png" alt=""></p><h3 id="4-3-预训练模型Pretrained和原始模型raw-models"><a href="#4-3-预训练模型Pretrained和原始模型raw-models" class="headerlink" title="4.3 预训练模型Pretrained和原始模型raw models"></a>4.3 预训练模型Pretrained和原始模型raw models</h3><p>与其他医学领域类似，DLR可以基于以下两种方法之一进行提取。</p><h4 id="4-3-1-新模型训练"><a href="#4-3-1-新模型训练" class="headerlink" title="4.3.1 新模型训练"></a>4.3.1 新模型训练</h4><p>&#8195;&#8195;从头开始训练深层网络以提取DLR的优势在于，网络可以完全根据眼前的具体问题进行调整。然而，由于过拟合和类别不平衡，新的训练模型预测效能可能有限。研究需要保护患者隐私，也需要专家提供真实标签以作为预测金标准，这些条件通常会限制可用于提取DLR的医学数据集的数量，从而导致深度学习网络的过拟合问题。第二个问题是类别不平衡问题，即正类和负类的数量不相等。当诊断为异常的患者数量通常少于健康受试者的可用数据量时，就会发生这种情况。通常情况下，由于类别失衡缘故，即阳性标签的数量少于阴性标签的数量，从而使分类器预测结果偏向于阴性类别。这样的预测结果比将阴性结果预测为阳性结果更糟糕。我们可以采取以下方法来解决这两个问题。</p><ol><li><p><strong>数据增强:可将对现有数据进行不同处理（如旋转），以生成新的样本进行训练。</strong>此外，subpatch扩展是放射组学中常采用的另一种增强形式，通过从原始图像中提取几个随机固定大小的<strong>subpatch</strong>来处理数据不足的情况。</p></li><li><p><strong>多任务训练：此方法用于处理类不平衡和数据不足问题，它通过减少参数的数量并因此降低过拟合的风险来实现。</strong>例如，在文献[44]中基于MRI数据训练多任务CNN模型，进行脊柱异常分类。这种情况下的多任务，是指通过相同的统一网络同时执行不同的分类任务。损失函数定义为与不同分类任务相关的所有损失的加权总和。在多任务学习中做出的一个重要决定是确定分支起始点，例如，在[44]中，训练了一个统一的CNN，其中共享所有卷积层来执行不同的分类任务，但任务从全连接层开始分离，后续进行各自的分类任务。</p></li><li><p><strong>损失函数修改：DLR提取中处理类别不平衡的另一种常用方法，是通过赋予少数类别更大的权重来修改损失函数。</strong></p></li></ol><h4 id="4-3-2-通过预先训练的网络进行迁移学习"><a href="#4-3-2-通过预先训练的网络进行迁移学习" class="headerlink" title="4.3.2 通过预先训练的网络进行迁移学习"></a>4.3.2 通过预先训练的网络进行迁移学习</h4><p>&#8195;&#8195;类别不平衡和训练数据不足的另一种解决方案是“迁移学习”，再进行“微调”。<strong>迁移学习阶段是指使用自然图像数据集训练深度学习网络。在微调阶段，将使用所需的医学数据集对训练的网络进行再训练。</strong>文献[42]中即采用了迁移学习方式，其中预训练的CNN用于基于乳房X线图像的乳腺癌分类。该研究所使用的预训练CNN模型是Alexnet，该算法过于复杂并且在小型数据集中易于过度拟合。因此，该网络首先使用Image Net数据库进行预训练，该数据库由100多万个自然图像组成。文献[46]中还采用了基于ImageNet的预训练CNN进行肺癌生存预测。</p><h3 id="4-4-放射组学中的深度学习框架"><a href="#4-4-放射组学中的深度学习框架" class="headerlink" title="4.4 放射组学中的深度学习框架"></a>4.4 放射组学中的深度学习框架</h3><p>&#8195;&#8195;可以通过判别性discriminative和/或生成性generative的深度学习网络提取放射组学特征。从其名称可以看出，判别性的深层网络模型想要提取使得不同类别(例如，正常的或癌性的)可区分的特征，因此，这些模型可以从所提取的特征直接对实例进行分类。另一方面，<strong>生成性模型是无监督的，即它们是在不考虑类别标签的情况下训练的。</strong>一般来说，这些模型的目标是了解数据分布，以便能够从同一分布生成新的数据。生成模型可以提取数据的自然和代表性特征，然后将其用作分类器的输入。此外，在放射组学领域，通常会训练生成模型，然后将学习到的权重用作判别模型的初始权重。在本节中，我们将介绍在放射组学中广泛使用的判别性和生成性深度学习网络模型。</p><h4 id="4-4-1-判别性模型"><a href="#4-4-1-判别性模型" class="headerlink" title="4.4.1 判别性模型"></a>4.4.1 判别性模型</h4><h5 id="4-4-1-1-CNN"><a href="#4-4-1-1-CNN" class="headerlink" title="4.4.1.1 CNN"></a>4.4.1.1 CNN</h5><p>&#8195;&#8195;CNN是结合了非线性激活函数和池化层的一堆执行卷积滤波的层网络。它最近取得的成果使其成为医学领域(如放射组学)的流行架构。CNN在整个输入中使用了共享权值，以至于减少了可训练参数的数量，让它在实际研究中更为实用。与手工提取特征不同，卷积层中使用的核不是预先确定的，而是通过训练过程自动学习的。因为该算法的灵活性且不需要先验知识即可应用，使得CNNs成为提取DLR特征的适用方法。在文献[55]中，由CNN提取的DLR投影到二维空间中<strong>可以在视觉上区分出肺肿瘤的良恶性，</strong>而原始的像素值完全达不到该区分效果</p><p>&#8195;&#8195;在放射组学领域采用CNN时，全连接层的输出通常被视为DLR特征。这些特征随后在原CNN内使用，以提供所需的(分类和/或回归)输出，如癌症类型。或也可存于网络，以作为输入提供给其余的放射组学分析流程。如文献[46]中，从CNN的分类(SoftMax)层前一层提取DLR，以预测肺癌生存期为目的。这些特征被称为预激活线性单位(preReLU)和后ReLU特征，因为它们是在应用ReLU激活函数之前和之后提取的。经过特征选择算法后，<strong>DLR特征用于四个分类器(即朴素贝叶斯、最近邻、DT（决策树）和随机森林（RF）)</strong>作为输入数据。</p><p><strong>&#8195;&#8195;放射组学中使用的CNN架构可以分为三大类：1)标准架构，2)自设计架构，3)多CNNs。</strong>在本节中，我们将描述这些类别，并包括放射组学的示例</p><p><strong>1. 标准CNN架构</strong></p><p>&#8195;&#8195;顾名思义，标准体系结构是那些以前为解决特定问题而设计的体系结构，由于它们的成功应用，现在正被用于放射组学研究。目前，LeNet和AlexNet两个架构用于放射组学中。<strong>LeNet是最简单的CNN架构之一，共有七层。</strong>然而，研究人员为获得更高的性能，已经对这个网络进行了相关修改。例如，在文献[10]中使用的CNN是具有总共九层的LeNet架构，包括三个卷积层、三个池化层和一个完全连接层，该架构将肺部肿瘤分类为良性或恶性。放射组学中另一个常用的标准架构是11层Alexnet，它在文献[42]中被用来从乳腺钼靶图像中提取DLR特征。从所有11层中提取特征，并用作11个支持向量机的输入，目的是将乳腺肿瘤分类为良性或恶性。由于不清楚DLR特征的哪一组(即11个底层的哪一个输出)更实用，因此研究对这些支持向量机进行比较，最终选择曲线下面积最大的一个模型进行乳腺癌的预测分析。文献[42]中的结果得出结论，从第9层(最后一个完全连接的层和分类层之前的完全连接的层)提取的特征是乳腺癌的最佳预测器，并且与先前的特征相比，它们维数更低，直接降低了计算成本。也就是说，该研究与[46]相反，它选择全连接层之前的最后一个卷积层的输出作为DLR特征。</p><p>&#8195;&#8195;虽然AlexNet是一个强大的网络，但它对小数据集来说参数太多，因此容易过度拟合。因此，文献[15]使用了五层的Alexnet，以避免过拟合问题。这个网络的输入是CT和正电子发射断层扫描(PET)图像的组合，每个图像都有<strong>三个通道：</strong>一个是由专家指定的对应于肺结节中心的切片，另外两个是相邻的切片。该研究的目的是将肺部肿瘤分为良性或恶性，虽然采用的CNN并没有比经典方法(即HCR)高得多的准确性，但它更为方便，因为它不需要对感兴趣区域进行分割，省了大量的人力和不稳定因素。</p><p>&#8195;&#8195;Inception network是放射组学中采用的另一种CNN。该网络包括在同一层内具有不同内核大小和池的并行卷积，其总体目标是允许网络学习最佳权重并选择最有用的特性。CNN最初用于检测糖尿病视网膜病变[76]。这篇文章是美国食品和药物管理局批准的第一篇基于深度学习的糖尿病视网膜病变检测工作。</p><p><strong>2. 自行设计的CNN</strong></p><p>&#8195;&#8195;与使用标准CNN研究者不同的是，有些研究者基于实际的放射组学问题设计了他们自己的CNN架构。例如，文献[41]使用了一个具有三个卷积层的CNN来提取DLR特征，虽然CNN本身是使用这些特征来训练肿瘤良恶性分类模型，但该研究中，通过CNN提取的DLR特征被用于决策树分类模型。</p><p>&#8195;&#8195;文献[43]也采用了类似的方式，使用一个带有6个卷积层和一个全连接层的CNN来提取DLR，从而解决了脑瘤的分类问题。然而，不同于前面的设计网络，该CNN是为肿瘤分割而开发的，而且“特征提取自最后一个卷积层，因为它们对输入的转化和标准化更具鲁棒性”。也就是说，该研究设计了用于分割的CNN，经过训练后，使用最后一个卷积层的输出作为DLR特征。<strong>由于提取特征的质量取决于感兴趣区域分割的准确性，当分割精确时，放射组学特征的质量是有保证的。</strong>由于分割的重要性，越来越多的CNN架构被开发出来，其中一个就是<strong>全卷积神经网络(FCNN)。</strong>在FCNN中，全连接层被重写为卷积层，如此便可不需要固定输入大小。将该网络扩展到三维图像分割中，可以同时分割多个目标。为了降低FP率，FCNN进一步与图形模型（如马尔可夫随机场和条件随机场）相结合。最后，为了提高输出分割图像的分辨率，提出了U-Nets模型，模型包括上行卷积以增加图像大小和跳过连接以恢复空间信息等步骤。</p><p>&#8195;&#8195;在文献[47]中也研究使用了CNNs用于肺癌检测，不同之处在于网络的输入。该CNN网络的输入不仅是原始图像，而且是结节增强和血管增强图像，这意味着为模型提供了更多关于肿瘤和血管的信息，以降低模型将这两者位置匹配错误的风险。这里主要重点是在保持灵敏度高的同时降低FP率。因此，一开始就选择了大量的结核患者为模型输入做准备。在[45]中，文献进一步研究了CNNs的使用。该研究基于下采样的体积CT图像及其分割结果，使用一个七层CNN架构用于寿命预测。在文献[48]中，提供了一种称为XmasNet的架构，可以最大限度地提高前列腺癌诊断的准确性。这个网络由四个卷积层、两个全连接层、两个池化层和一个用于癌症预测的SoftMax层组成。这个网络的输入是3D MRI图像。</p><p>&#8195;&#8195;总之，<strong>自行设计的CNN是通过改变网络的深度（卷积层和非卷积层的数量）、层叠的顺序、网络的输入类型（例如，单通道或不同形式的多通道）和/或输出被视为DLR特征的层来开发的。</strong></p><p><strong>3. 多种CNN</strong></p><p>&#8195;&#8195;除了使用标准或自行设计的CNN外，一些研究人员还提出使用多个网络的CNN。这些网络受益于多个不同的输入，且不同的网络具有不同的模式、范围和角度（图6）或具有不同性质的不同架构。“范围”是设计输入结构时需要考虑的一个重要因素。例如，为了区分肿瘤和血管，输入切片中应包含足够大的区域。而要区分实体肿瘤和非滑动肿瘤，则应以结节区为主要核心。考虑到这一点，文献[59]设计了一个用于肺部肿瘤分类的CNN架构，其中输入不仅来自不同角度（矢状、冠状和轴向）的影像，而且影像包含不同的范围。按照类似的思路，文献[50]还设计了一个多CNN架构，其中每个CNN以特定比例的肺部肿瘤切片（见图6）作为输入，并生成相关的DLR特征。然后将从所有CNN中提取的特征串接起来，通过一个传统的分类器（即SVM）用于肺癌的恶性预测。此外，使用肿瘤切片不仅可以提供关于肿瘤本身的信息，而且还可以提供肿瘤周围组织的信息。由于肿瘤大小在患者之间存在很大的差异，因此使用不同范围的切片代替单个切片将提高提取的DLR特征的整体性能。此外，这类多CNN体系结构的一个特性是，由于多CNN架构共享评估参数，所以可以在合理时间内进行模型的有效训练。使用多个CNN架构的另一个好处是，对于输入中添加的小噪声数据使得网络更具鲁棒性。</p><p><img src="https://i.loli.net/2020/12/21/I6erHlNASpRMw8T.png" alt="图6 CT扫描下肺的不同角度以及三种不同比例下的肺肿瘤范围"></p><p>&#8195;&#8195;与文献[50]中的研究类似，文献[56]设计了一个多视图CNN模型。它使用七个不同范围的切片作为输入，不同的是这些切片的大小被调整为相同的尺寸。因此，可以使用单个CNN替代多个CNN。<strong>这项工作还将肺肿瘤的二分类扩展到三分类：</strong>肺肿瘤分为良性、原发性恶性和转移性恶性。此外，除了精度和AUC等常用评价指标外，本文还采用了另一种被称为可分性的验证方法。可分性是指不同类别基于其学习特征的可区分程度，根据前述文章，所提出的多视图CNN比单尺度CNN具有更高的可分性。除此之外，随着层数加深，可学习具有更高可分性的特征。</p><p>&#8195;&#8195;在文献[55]中，通过设计一种称为多裁剪CNN的新颖CNN架构，进一步扩展了使用多尺度图像切片的思想。在这种结构中，<strong>通过并行池化层提取多尺度特征，</strong>而不是在不同尺度上获取输入特征。其中一层是将池化应用于前一层输入的裁剪版本，然后将来自多个池化层的特征连接起来并输送到下一层。三维肺部CT图像是该网络的输入，由于多个CNN被一个CNN代替，因此可以节省更多训练时间。除了预测肺部肿瘤的恶性程度，该研究还预测了与肿瘤相关的其他属性(如直径)，<strong>方法是用回归层替换最终的SoftMax层。</strong>请注意，此网络并未同时执行所有任务，而是一个一个地执行，这使该网络有别于“预处理或原材料”一节中讨论的多任务训练网络。</p><p>&#8195;&#8195;在[51]中，通过多个CNN的放射组学进一步探讨了使用MRI诊断阿尔茨海默病。其中，在CNN的第一阶段，根据正常和异常大脑的比较检测出几个标志点。这些标志点随后被用来提取切片，因此，每一个CNN都是通过将对应于特定标志点的切片作为输入来训练的。最后的决策是根据所有CNN结果进行投票得出的。这里，使用多重架构背后的想法是，检测阿尔茨海默病需要检查大脑的不同区域。</p><p>&#8195;&#8195;总之，为DLR特征提取而开发的多种CNN方法是通过融合基于特定输入训练的多个CNN输出，或者是在单个网络中嵌入多路径层来修改前一层的输出来设计的。</p><p><strong>&#8195;&#8195;上述所有CNN架构的一个共同挑战是，它们没有考虑对象之间的空间信息。</strong>例如，他们可能没有考虑到组织内异常的位置作为其类型的指标。下一节中新提出的称为CapsNets的深层架构就是为了克服这个缺点而引入的。</p><h5 id="4-4-1-2-CapsNets"><a href="#4-4-1-2-CapsNets" class="headerlink" title="4.4.1.2 CapsNets"></a>4.4.1.2 CapsNets</h5><p><strong>&#8195;&#8195;尽管CNN在许多医学和非医学分类问题中是最先进的，但它们易受若干缺点的影响，包括模型可解释性低及忽略图像元素之间的空间关系导致错误分类等。此外，CNN在对某些类型的转换时鲁棒性较低。与池层相关的空间关系信息丢失由新提议的CapsNets解决，该CapsNets由卷积层和主囊层组成，可以处理更多类型的转换。这些深层体系结构能够考虑对象位置之间的关系，并通过其按协议路由过程容忍更多类型的转换，这表明不会将对象分类为特定类别，除非此对象的较低级别的元素同意该类别的存在。CapsNets的另一个重要特性是，它们可以处理较小的数据集，而大多数医疗领域研究数据量较小，因此适用性更强。在此，我们将解释CapsNets的体系结构(如图7所示)，以及它们的处理过程。</strong></p><p><img src="https://i.loli.net/2020/12/21/NQ9HEua1voJ8zpj.png" alt="图7 CapsNets架构。使用卷积层来形成主囊层，并且基于这些囊层之间的一致性来做出决定。Conv:卷积。"></p><p><strong>囊层是一组神经元，其活动向量由不同的实例化参数组成，活动向量的长度代表了特定实例出现的概率。</strong>初级囊层中的每个囊都试图预测下一层（母囊）中所有囊的结果。但是，这些预测以不同的系数输送到下一层，该系数基于实际输出与预测的一致程度。这种在耦合囊层之前寻找一致性的过程则称为协议路由。这是CapsNets最重要的特性，它使囊层考虑对象之间的空间关系，因此对几种类型的变换（如仿射变换和旋转）具有鲁棒性。将$u_i$定义为初级胶囊层中胶囊<strong>i</strong>的输出，胶囊$j$、$u_{j|i}$的预测计算如下，其中$W_{ij}$是反向传播中要学习的权重矩阵：</p><script type="math/tex; mode=display">u_{j|i}=W_{ij}u_i\tag7</script><p>因此，$C_{ij}$，即囊层i和j的耦合系数，是根据这两个囊层之间的构象程度来计算的。基于这样一个想法，即如果两个载体一致，它们将有一个更大的内积，并且母囊层$j$，$S_j$的输出可按一下公式进行估计：</p><script type="math/tex; mode=display">S_j=C_{ij}u_{j|i}\tag8</script><p>最后，将一个非线性函数应用于$S_j$，使其值始终保持不大于1（存在概率）。</p><p>&#8195;&#8195;在本节中，我们进一步研究了CapsNets在放射组学中的初步应用。[37]探索了各种CapsNet架构以选择最大化预测精度的架构，从而导致与原始囊网络（256个滤波器）相比，选定的网络具有更少的卷积滤波器（64个滤波器）。该结构由一个卷积，一个主囊层和一个分类层组成，最终精度达到86.56%。</p><p>&#8195;&#8195;此外，基于两种类型的输入分别训练单独的网络：原始脑图像和分割的肿瘤区域，这意味着CapsNet在用肿瘤勾画结果进行训练时表现更好。原因可能是因为脑图像背景较为复杂，因此分散网络可以更好的提取重要的区别特征。CapsNet对于两种输入类型的准确率都高于CNN, CNN对于肿瘤图像的准确率为78%。几个因素可能使CapsNet分类性能更好，包括它处理小数据集的能力，以及对转换和旋转的鲁棒性方面。</p><h5 id="4-4-1-3-RNNs"><a href="#4-4-1-3-RNNs" class="headerlink" title="4.4.1.3 RNNs"></a>4.4.1.3 RNNs</h5><p>&#8195;&#8195;大多数深度网络架构需要固定大小的输入，这使得它们对于体积图像的放射组学分析（即体积级分类）无效，例如，当必须一次处理整个体积时（例如基于3D体积的肿瘤分类）。在这些情况下，可以采用RNN，因为它们能够处理诸如CT或MR切片的连续数据，并且它们将当前图像切片以及处理先前图像切片的结果作为输入。同时，RNN也可用于监测随访检查获得的医学图像（即患者级分类）。</p><p>&#8195;&#8195;由于RNN与梯度消失问题有关，因此提出了一种称为长短期记忆（long-short-term-memory , LSTM）的模型，它能够决定存储什么和忘记什么。尽管RNN和LSTM在计算上似乎比其他体系结构要求更高，但通过在整个网络上使用相同的权重，它们的训练时间和成本大大降低。基于超声图像序列，文献[61]使用LSTMs用于的前列腺癌的良恶性分类，其中使用序列进行分类的预测准确性高于基于单个图像的预测准确性。</p><h4 id="4-4-2-生成式模型"><a href="#4-4-2-生成式模型" class="headerlink" title="4.4.2 生成式模型"></a>4.4.2 生成式模型</h4><p>&#8195;&#8195;大多数深度生成模型的目标是从数据分布中学习抽象但丰富的特征，以从相同的分布中生成新的样本。<strong>这些生成模型在放射组学中实用的原因是，模型所学习的特征可能最能描述实际数据集，因此有可能作为放射组学特征并有助于诸如肿瘤分类的后续任务</strong>。自动编码器网络（Autoencoder networks)、深度信念网络(deep belief networks)和深度玻尔兹曼机器(deep Boltzmann machines, DBMs)是放射组学中使用的深度生成模型。</p><p><strong>1. 自动编码器网络</strong></p><p>&#8195;&#8195;自动编码器网络由两个主要组件组成：一个编码器，它将有$f^{(i)}(1\leq i \leq N_s)$表示的$N_s$医学图像作为输入，并将每个图像转换为潜在空间$\phi(Wf^{(i)}+b)$，即放射组学特征。第二个组件，即解码器，利用潜在空间并试图重建输入图像，目标是最小化原始输入与重建输入之间的差异$\phi((W^T\phi(Wf^{(i)}+b))+c)$,由下式给出：</p><script type="math/tex; mode=display">\underset {W,b,c}{min}\sum_{i=1}^{N_s}\lVert\phi((W^T\phi(Wf^{(i)}+b))+c)-f^{(i)} \rVert\tag9</script><p>其中$\phi(.)$是网络的激活函数，$W$表示编码器和解码器使用的网络的权重矩阵，$b$表示编码器偏置矢量，$c$表示解码器偏置矢量，上标$T$表示转置算子。编码变量可以被当做放射组学特征的原因是它们是可用于再现它的输入图像的最重要的代表。虽然一个自动编码器可以完全地进行端到端的训练，但是为了从良好的初始权值开始训练，从而避免梯度消失的问题，我们可以先逐层训练，然后以得到的权值作为自编码器的起始点。根据应用条件的不同，自动编码器有几个不同的扩展。</p><p>1) 去噪自动编码器</p><p>&#8195;&#8195;为了使自动编码器能够捕捉到更健壮的特征，可以在输入特征中添加一些噪声，这种自动编码器被称为去噪自动编码器（DAE）。在文献[57]中，DAE被用于提取放射组学特征，然后将这些特征用于支持向量机进行肺部肿瘤的良恶性鉴别。文献[10]还采用了一种五层去噪的自动编码器，它以损坏的肺部图像为输入，目的是恢复原始图像。此外，由该网络编码器提取的400个特征被做为放射组学特征来训练另一个肺癌分类（识别肿瘤的类型，例如，良性或恶性）模型。</p><p>2) 卷积自动编码器</p><p>&#8195;&#8195;由于考虑了空间相关性，卷积自动编码器对放射组学(图像型输入)特别有用。在这些网络中，相邻节点共享相同的权重。在文献[11]中采用了具有五个卷积层的卷积自动编码器用于肺癌诊断(即，识别癌症的存在)。</p><p>&#8195;&#8195;放射组学中有两种利用自动编码器的常用策略。第一种也是最常见的方法是直接使用提取的特征来训练分类器。例如，文献[64]使用五层自动编码器提取了放射组学特征，该编码器使用分割的感兴趣区域作为输入。这些特征随即作为决策树的输入以实现分类目的，这种情况下，输出即为不同分类的肺结节。自动编码器还可以用于影像预处理，使网络在执行实际分类任务之前提取代表性放射组学特征。例如，文献[60]首先基于调整大小的(向下采样图像以便于训练)肺部CT影像来训练DAE。接着，在网络中加入一个分类层，以调整后的图像作为输入，对整个网络重新训练。</p><p><strong>2. 深度信念网络</strong></p><p><strong>&#8195;&#8195;深度信念网络（Deep belief networks,DBN）是相互堆叠的受限玻尔兹曼机器（restricted Boltzmann machines, RBM），其中RBM是一个无监督的两层随机神经网络，可以以最小化重建误差为目标对概率依赖性进行建模。</strong>更重要的是，RBM是一个二分图，允许值在两个方向上传播。虽然DBNs由RBMs组成，但只有最上面的两层是无向关系。DBN首先以贪婪的方式进行训练，这意味着RBM子网被单独训练，然后再进行轻微调动。在文献[10]中，设计了一个由四个隐藏层组成的DBN，其目标是从1600个节点的顶层提取DLR。最后一层与外部神经网络相连，用于对肺结节进行分类。此外，为了获得多通道输入(即原始图像、分割的肿瘤和梯度图像)，这些通道在被输送到网络之前按矢量方式进行连接。</p><p><strong>3. 深玻尔兹曼机器(DBMs)</strong></p><p>&#8195;&#8195;DBM同DBN一样，也是由RBM组成，但与DBN的不同之处在于，尽管它们是以逐层方式训练的，但DBMs包括了所有层之间的无向关系，以至于DBMs在计算上无效。然而，由于双向关系，RBM可以从数据中捕获复杂的模式。DBM在[34]中被用于阿尔茨海默病的诊断。在这项工作中，再DBM的最后一层添加了分类层，如此，它不仅可以提取分层（生成）特征，还可以提取判别特征。</p><p>&#8195;&#8195;以上，本文就完成了放射组学工作流程中使用的不同深度判别模型和生成模型的概述。接下来，我们考虑这种体系结构的关键缺点，即充当黑匣子。</p>]]></content>
    
    
    <categories>
      
      <category>radiomics</category>
      
      <category>medical imaging</category>
      
    </categories>
    
    
    <tags>
      
      <tag>radiomics</tag>
      
      <tag>literature study</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Support Vector Machine:Machine Learning Classification Algorithms 2——Supplement</title>
    <link href="/2020/11/25/SVM/"/>
    <url>/2020/11/25/SVM/</url>
    
    <content type="html"><![CDATA[<h1 id="支持向量机涉及的知识点整理"><a href="#支持向量机涉及的知识点整理" class="headerlink" title="支持向量机涉及的知识点整理"></a>支持向量机涉及的知识点整理</h1><p><a href="http://hanzawa.me/2020/08/26/Support-Vector-Machine-Machine-Learning-Classification-Algorithms-2/">Hanzawa 的支持向量机</a></p><p>在没有全面接触机器学习之前，神经网络、贝叶斯分类器、支持向量机是我最先接触的几个方法，当然前两者是在NUS系统学习过，而支持向量机只是听说过，所以本以为支持向量机的理论推导应该不难，结果被打脸，看的我云山雾罩。对于其涉及的优化知识点认知为零。因此整理本学习内容，作为支持向量机的基础补充。(很多的公式，对我来说是个挑战)</p><h2 id="1-拉格朗日乘子法"><a href="#1-拉格朗日乘子法" class="headerlink" title="1. 拉格朗日乘子法"></a>1. 拉格朗日乘子法</h2><h3 id="1-1等式约束优化问题"><a href="#1-1等式约束优化问题" class="headerlink" title="1.1等式约束优化问题"></a>1.1等式约束优化问题</h3><p>拉格朗日乘子法（Lagrange multipliers)可以将有$d$个变量和$k$个约束条件的最优化问题转化成有$d+k$个变量的无约束最优化问题求解。</p><p>先考虑一个等式约束的优化问题，假定$x$为$d$维向量，欲寻找$x$的某个取值$\ x^*$，使目标函数$ f(x)$最小且同时满足$g(x) = 0$的约束。从几何角度看，该问题的目标是在由方程$g(x) = 0$确定的$d-1$维曲面上寻找能使目标函数$f(x)$最小化的点，此时不难得到如下结论：</p><ul><li>对于约束曲面上的任一点$x$,该点的梯度$\nabla g(x)$正交于约束曲面；</li><li>在最优点$x^*$，目标函数在该点的梯度$\nabla f(x^*)$正交于约束曲面。</li></ul><p>由此可知，在最优点$x^*$，如图B.1所示，梯度$\nabla g(x)$和$\nabla f(x)$的方向必相同或相反，即存在$\lambda \neq 0$ 使得</p><script type="math/tex; mode=display">\begin{align}\ \nabla f(x^*) + \lambda \nabla g(x^*) = 0        \end{align}\tag1</script><p><img src="https://i.loli.net/2020/11/26/fE2greaUWNvmK3y.jpg" alt=""></p><p>$\lambda$称为lagrange乘子，定义拉格朗日函数</p><script type="math/tex; mode=display">L(\boldsymbol  x,\boldsymbol \lambda)=f(\boldsymbol x)+\lambda g(\boldsymbol x)\tag2</script><p>因此，将其对$x$的偏导数$\nabla_x L(x,\lambda)$置零即得到式(1)，同时，将其对$\lambda$的偏导数$\nabla_\lambda L(x,\lambda)$置零即得到约束条件$g(x)=0$。于是，原约束化问题可转化为对拉格朗日函数$L(x,\lambda)$的无约束优化问题。</p><h2 id="2-不等式约束优化问题"><a href="#2-不等式约束优化问题" class="headerlink" title="2.不等式约束优化问题"></a>2.不等式约束优化问题</h2><p>现在考虑不等式约束$g(x) \leq 0$，如上图所示，此时最优化点$x^*$或在$g(x)<0$的区域中，或在边界$g(x)=0$上。对于$g(x) < 0$的情形，约束$g(x) \leq 0$不起作用，可直接通过条件$\nabla f(x)=0$来获得最优点；这等价于将$\lambda$置零后对$\nabla_x L(x,\lambda)$置零得到最优点。$g(x)=0$的情形类似与上面等式约束的分析，但须注意的是，此时$\nabla f(x^*)$的方向必与$\nabla g(x^*)$相反，即存在常数$\lambda >0$ 使得$\nabla f(x^*) + \lambda \nabla g(x^*) = 0$ 。因此整合这两种情形，必满足$\lambda g(x) =0$。因此，在约束$g(x) \leq 0$下最小化$f(x)$，可转化为在如下约束下最小化式（2）的拉格朗日函数：</p><script type="math/tex; mode=display">\begin{cases}g(x)\leq 0\\\lambda \geq 0\\\mu_jg_j(x)=0\end{cases}\tag3</script><p>式(3)称为Karush-Kuhn-Tucker（简称KKT）条件。</p><p>上述做法可推广到多个约束，考虑具有$m$个等式约束和$n$个不等式约束，其可行域$\boldsymbol  D \subset \boldsymbol R^d$非空的优化问题</p><script type="math/tex; mode=display">\begin{align}\min_x \space & f(x)\\\text{s.t.} \space & h_i(x)=0 \space (i=1,\cdots,m),\\&g_j(x) \leq 0 \space (j=1,\cdots,n)\end{align}\tag4</script><p>引入拉格朗日乘子$\boldsymbol\lambda = (\lambda_1,\lambda_2,\cdots,\lambda_n)^\top$和$\boldsymbol\mu=(\mu_1,\mu_2,\cdots,\mu_m)^\top$，相应的广义拉格朗日函数 (generalized Lagrange function) 为：</p><script type="math/tex; mode=display">L(\boldsymbol x,\boldsymbol\lambda,\boldsymbol\mu)=f(\boldsymbol x)+\sum_{j=1}^m \lambda_j g_j(\boldsymbol x)+\sum_{i=1}^n \mu_i h_i(\boldsymbol x)</script><p>其中$\lambda_j$，$\mu_i$被称作是拉格朗日乘子，$\lambda_j \geq 0$。</p><p>由不等式约束引入的KKT条件($j=1,2,\cdots,n$)为</p><script type="math/tex; mode=display">\begin{cases}g_j(x)\leq 0\\\mu_j \geq 0\\\mu_jg_j(x)=0\end{cases}\tag5</script><h2 id="未完待续……"><a href="#未完待续……" class="headerlink" title="未完待续……"></a>未完待续……</h2><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><blockquote><ul><li>叶其孝、沈永欢《实用数学手册》（第二版），科学出版社，2011.8。</li><li>周志华《机器学习》，清华大学出版社，2016.2</li><li><a href="https://blog.pluskid.org/?p=702">https://blog.pluskid.org/?p=702</a></li><li></li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>machine learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>machine learning</tag>
      
      <tag>SVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Reproduce: medical imaging database</title>
    <link href="/2020/11/16/reproduce-medical-imaging-database/"/>
    <url>/2020/11/16/reproduce-medical-imaging-database/</url>
    
    <content type="html"><![CDATA[<h1 id="转载：【数据集】一文道尽医学图像数据集与竞赛"><a href="#转载：【数据集】一文道尽医学图像数据集与竞赛" class="headerlink" title="转载：【数据集】一文道尽医学图像数据集与竞赛"></a>转载：<a href="https://zhuanlan.zhihu.com/p/50615907">【数据集】一文道尽医学图像数据集与竞赛</a></h1><p>作者：<a href="https://www.zhihu.com/people/long-peng-11">言有三</a></p><p>在AI与深度学习逐渐发展成熟的趋势下，人工智能和大数据等技术开始进入了医疗领域，它们把现有的一些传统流程进行优化，大幅度提高各种流程的效率、精度、用户体验，同时也缓解了医疗资源的压力和精确度不够的问题。</p><h2 id="01-医学数据集"><a href="#01-医学数据集" class="headerlink" title="01 医学数据集"></a>01 <strong>医学数据集</strong></h2><p>智能医疗有很多的发展方向，例如医学影像处理、诊断预测、疾病控制、健康管理、康复机器人、语音识别病历电子化等。当前人工智能技术新的发力点中的医学图像在疾病的预测和自动化诊断方面有非常大的意义，本篇即针对医学影像中的病例分析，降噪，分割，检索等领域来介绍一些常用的数据集。</p><p><strong>1.1 病例分析数据集</strong></p><p><strong>1.1.1 ABIDE</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//preprocessed-connectomes-project.org/abide/">http://preprocessed-connectomes-project.org/abide/</a></p><p>发布于2013年，这是一个对自闭症内在大脑结构的大规模评估数据集，包括539名患有ASD和573名正常个体的功能MRI图像。</p><p><strong>1.1.2 OASIS</strong></p><p>数据集地址：<a href="https://link.zhihu.com/?target=http%3A//www.oasis-brains.org/">http://www.oasis-brains.org/</a></p><p>OASIS，全称为Open Access Series of Imaging Studies，已经发布了第3代版本，第一次发布于2007年，是一项旨在使科学界免费提供大脑核磁共振数据集的项目。它有两个数据集可用，下面是第1版的主要内容。</p><p>(1) 横截面数据集：年轻，中老年，非痴呆和痴呆老年人的横断面MRI数据。该组由416名年龄在18岁至96岁的受试者组成的横截面数据库组成。对于每位受试者，单独获得3或4个单独的T1加权MRI扫描包括扫描会话。受试者都是右撇子，包括男性和女性。100名60岁以上的受试者已经临床诊断为轻度至中度阿尔茨海默病。</p><p>(2) 纵向集数据集：非痴呆和痴呆老年人的纵向磁共振成像数据。该集合包括150名年龄在60至96岁的受试者的纵向集合。每位受试者在两次或多次访视中进行扫描，间隔至少一年，总共进行373次成像。对于每个受试者，包括在单次扫描期间获得的3或4次单独的T1加权MRI扫描。受试者都是右撇子，包括男性和女性。在整个研究中，72名受试者被描述为未被证实。包括的受试者中有64人在初次就诊时表现为痴呆症，并在随后的扫描中仍然如此，其中包括51名轻度至中度阿尔茨海默病患者。另外14名受试者在初次就诊时表现为未衰退，随后在随后的访视中表现为痴呆症。</p><p><strong>1.1.3 DDSM</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//marathon.csee.usf.edu/Mammography/Database.html">http://marathon.csee.usf.edu/Mammography/Database.html</a></p><p>发布于2000年，这是一个用于筛选乳腺摄影的数字数据库，是乳腺摄影图像分析研究社区使用的资源。该项目的主要支持来自美国陆军医学研究和装备司令部的乳腺癌研究计划。DDSM项目是由马萨诸塞州综合医院（D. Kopans，R. Moore），南佛罗里达大学（K. Bowyer）和桑迪亚国家实验室（P. Kegelmeyer）共同参与的合作项目。数据库的主要目的是促进计算机算法开发方面的良好研究，以帮助筛选。数据库的次要目的可能包括开发算法以帮助诊断和开发教学或培训辅助工具。该数据库包含约2,500项研究。每项研究包括每个乳房的两幅图像，以及一些相关的患者信息（研究时间，ACR乳房密度评分，异常微妙评级，异常ACR关键字描述）和图像信息（扫描仪，空间分辨率等）。包含可疑区域的图像具有关于可疑区域的位置和类型的像素级“地面真实”信息。</p><p><strong>1.1.4 MIAS</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//peipa.essex.ac.uk/pix/mias/all-mias.tar.gz">http://peipa.essex.ac.uk/pix/mias/all-mias.tar.gz</a></p><p><a href="https://link.zhihu.com/?target=https%3A//www.repository.cam.ac.uk/handle/1810/250394%3Fshow%3Dfull">https://www.repository.cam.ac.uk/handle/1810/250394?show=full</a></p><p>MIAS全称为MiniMammographic Database，是乳腺图像数据库。</p><p>乳腺MG数据（Breast Mammography）有个专门的database，可以查看很多数据集，链接地址为：<a href="https://link.zhihu.com/?target=http%3A//www.mammoimage.org/databases/">http://www.mammoimage.org/databases/</a></p><p><strong>1.1.5 MURA</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//stanfordmlgroup.github.io/competitions/mura/">https://stanfordmlgroup.github.io/competitions/mura/</a></p><p>发布于2018年2月，吴恩达团队开源了 MURA 数据库，MURA 是目前最大的 X 光片数据库之一。该数据库中包含了源自14982项病例的40895张肌肉骨骼X光片。1万多项病例里有9067例正常的上级肌肉骨骼和5915例上肢异常肌肉骨骼的X光片，部位包括肩部、肱骨、手肘、前臂、手腕、手掌和手指。每个病例包含一个或多个图像，均由放射科医师手动标记。全球有超过17亿人都有肌肉骨骼性的疾病，因此训练这个数据集，并基于深度学习检测骨骼疾病，进行自动异常定位，通过组织器官的X光片来确定机体的健康状况，进而对患者的病情进行诊断，可以帮助缓解放射科医生的疲劳。</p><p>公开可用的医学射线照相图像数据集概述</p><p><img src="https://pic2.zhimg.com/80/v2-ffe9bc0a1df80ac718ad9cc96a4ac209_720w.jpg" alt="img"></p><p>参考2018年论文：MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs.</p><p><strong>1.1.6 ChestX-ray14</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/nih-chest-xrays/data">https://www.kaggle.com/nih-chest-xrays/data</a></p><p><a href="https://link.zhihu.com/?target=https%3A//nihcc.app.box.com/v/ChestXray-NIHCC">https://nihcc.app.box.com/v/ChestXray-NIHCC</a></p><p>参考论文：</p><p>CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</p><p>ChestX-ray14 是由NIH研究院提供的，其中包含了30,805名患者的112,120个单独标注的14种不同肺部疾病（肺不张、变实、浸润、气胸、水肿、肺气肿、纤维变性、积液、肺炎、胸膜增厚、心脏肥大、结节、肿块和疝气）的正面胸部 X 光片。研究人员对数据采用NLP方法对图像进行标注。利用深度学习的技术早期发现并识别胸透照片中肺炎等疾病对增加患者恢复和生存的最佳机会至关重要。</p><p><strong>1.1.7 LIDC-IDRI</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI">https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI</a></p><p>LIDC-IDRI数据集是由美国国家癌症研究所(National Cancer Institute)发起收集的，目的是为了研究高危人群早期肺结节检测。该数据集中，共收录了1018个研究实例。对于每个实例中的图像，都由4位经验丰富的胸部放射科医师进行两阶段的诊断标注。该数据集由胸部医学图像文件(如CT、X光片)和对应的诊断结果病变标注组成。</p><p><strong>1.1.8 LUNA16</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//luna16.grand-challenge.org/Home/">https://luna16.grand-challenge.org/Home/</a></p><p>发布于2016年，是肺部肿瘤检测最常用的数据集之一，它包含888个CT图像，1084个肿瘤，图像质量和肿瘤大小的范围比较理想。数据分为10个subsets，subset包含89/88个CT scan。</p><p>LUNA16的CT图像取自LIDC/IDRI数据集，选取了三个以上放射科医师意见一致的annotation，并且去掉了小于3mm的肿瘤，所以数据集里不含有小于3mm的肿瘤，便于训练。</p><p><strong>1.1.9 NSCLC</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//wiki.cancerimagingarchive.net/display/Public/NSCLC%2BRadiogenomics">https://wiki.cancerimagingarchive.net/display/Public/NSCLC+Radiogenomics</a></p><p>发布于2018年，来自斯坦福大学。数据集来自211名受试者的非小细胞肺癌（NSCLC）队列的独特放射基因组数据集。该数据集包括计算机断层扫描（CT），正电子发射断层扫描（PET）/ CT图像。创建该数据集是为了便于发现基因组和医学图像特征之间的基础关系，以及预测医学图像生物标记的开发和评估。</p><p><strong>1.1.10 DeepLesion</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//nihcc.app.box.com/v/DeepLesion">https://nihcc.app.box.com/v/DeepLesion</a></p><p>DeepLesion由美国国立卫生研究院临床中心（NIHCC）的团队开发，是迄今规模最大的多类别、病灶级别标注临床医疗CT图像开放数据集。在该数据库中图像包括多种病变类型，目前包括4427个患者的32,735 张CT图像及病变信息，同时也包括肾脏病变，骨病变，肺结节和淋巴结肿大。DeepLesion多类别病变数据集可以用来开发自动化放射诊断的CADx系统。</p><p><strong>1.1.11 ADNI</strong></p><p>数据集地址：</p><p>http : //<a href="https://link.zhihu.com/?target=http%3A//adni.loni.usc.edu/data-samples/access-data/">http://adni.loni.usc.edu/data-samples/access-data/</a></p><p>ANDI涉及到的数据集包括如下几部分Clinical Data（临床数据）、MR Image Data（磁共振成像）、Standardized MRI Data Sets、PET Image Data（正电子发射计算机断层扫描）、Gennetic Data（遗传数据）、Biospecimen Data（生物样本数据）。</p><p><strong>1.2 医学降噪数据集</strong></p><p><strong>1.2.1 BrainWeb数据集</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//brainweb.bic.mni.mcgill.ca/brainweb/">http://brainweb.bic.mni.mcgill.ca/brainweb/</a></p><p>发布于1997年，这是一个仿真数据集，用于医学图像降噪。研究者可以截取不同断层的正常脑部仿真图像，包括T1，T2，PD3种断层，设置断层的厚度，叠加高斯噪声或者医学图像中常见的莱斯噪声，最终会得到181×217大小的噪声图像。</p><p><strong>1.3 医学分割数据集</strong></p><p><strong>1.3.1 DRIVE数据集</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//www.isi.uu.nl/Research/Databases/DRIVE/download.php">http://www.isi.uu.nl/Research/Databases/DRIVE/download.php</a></p><p>发布于2003年，这是一个用于血管分割的数字视网膜图像数据集，它由40张照片组成，其中7张显示出轻度早期糖尿病视网膜病变迹象。</p><p><strong>1.3.2 SCR数据集</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//www.isi.uu.nl/Research/Databases/SCR/">http://www.isi.uu.nl/Research/Databases/SCR/</a></p><p>发布于2000年，胸部X光片的分割，胸部X光片中解剖结构的自动分割对于这些图像中的计算机辅助诊断非常重要。SCR数据库的建立是为了便于比较研究肺野，心脏和锁骨在标准的后胸前X线片上的分割。</p><p>本着合作科学进步的精神，我们可以自由共享SCR数据库，并致力于在这些分割任务上维护各种算法结果的公共存储库。在这些页面上，可以在下载数据库和上载结果时找到说明，并且可以检查各种方法的基准结果。</p><p><strong>1.3.3 医学图像分析benchmark</strong></p><p>在网址<a href="https://link.zhihu.com/?target=https%3A//grand-challenge.org/challenges/">https://grand-challenge.org/challenges/</a>提供了时间跨度超过10年的医学图像资料。</p><p><strong>1.3.4 Ardiac MRI</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//www.cse.yorku.ca/~mridataset/">http://www.cse.yorku.ca/~mridataset/</a></p><p>ardiac MRI 是心脏病患者心房医疗影像数据，以及其左心室的心内膜和外膜的图像标注。包括33位患者案例，每个受试者的序列由沿着长的20帧和8-15个切片组成，共7980张图像。</p><p><strong>1.3.5 NIH</strong></p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=https%3A//www.kaggle.com/nih-chest-xrays">https://www.kaggle.com/nih-chest-xrays</a></p><p>发布于2017年，这是一个胸部X射线数据集，包含30,805个患者，14个疾病图像标签（其中每个图像可以具有多个标签），112,820个正面X射线图像，标签是使用自然语言处理从相关的放射学报告中自动提取。十四种常见的胸部病变包括肺不张，巩固，浸润，气胸，水肿，肺气肿，纤维化，积液，肺炎，胸膜增厚，心脏扩大，结节，肿块和疝。由于许多原因，原始放射学报告（与这些胸部X射线研究相关）并不是公开分享的。所以文本挖掘的疾病标签预计准确度 &gt; 90％，这个数据集适合做半监督的学习。</p><p><strong>1.4 List of Open Access</strong></p><p>在List of Open Access Medical Imaging Datasets网站上可以看到更多的相关方向的数据集。</p><p>数据集地址：</p><p><a href="https://link.zhihu.com/?target=http%3A//www.radrounds.com/profiles/blogs/list-of-open-access-medical-imaging-datasets">http://www.radrounds.com/profiles/blogs/list-of-open-access-medical-imaging-datasets</a></p><p><img src="https://pic4.zhimg.com/80/v2-20fa305e3a88623db367663e96eb001f_720w.jpg" alt="img"></p><p><img src="https://pic3.zhimg.com/80/v2-272b4b4f4cae4c25196521bbf4346c52_720w.jpg" alt="img"></p><h2 id="02-医学竞赛"><a href="#02-医学竞赛" class="headerlink" title="02 医学竞赛"></a>02 <strong>医学竞赛</strong></h2><p><strong>2.1 VISCERAL</strong></p><p>地址链接：http：//www.visceral.eu/</p><p>VISCERAL 是Visual Concept Extraction Challenge in Radiology的缩写，是放射学中的视觉概念提取挑战赛。他们提供几种不同成像模式（例如CT和MR）的几种解剖结构（例如肾，肺，膀胱等）的放射学数据以及一个云计算实例。</p><p><strong>2.2 Grand Challenges</strong></p><p>地址链接：</p><p><a href="https://link.zhihu.com/?target=https%3A//grand-challenge.org/challenges/">https://grand-challenge.org/challenges/</a></p><p>提供了医学图像分析领域内所有挑战的概述，下面举的例子是2019年的医学图像方面将要举办的竞赛。</p><p><img src="https://pic1.zhimg.com/80/v2-8b25c10e60e5abab2ef830ab2f3f7a0c_720w.jpg" alt="img"></p><p><img src="https://pic4.zhimg.com/80/v2-1b173440d06a03fd2c44727020a24257_720w.jpg" alt="img"></p><p><strong>2.3 Dream Challenges</strong></p><p>地址链接：http : //<a href="https://link.zhihu.com/?target=http%3A//dreamchallenges.org/">http://dreamchallenges.org/</a></p><p>这个挑战赛中包括有数字乳腺摄影梦想挑战；ICGC-TCGA DREAM体细胞突变称为RNA挑战（SMC-RNA）等等。</p><p><img src="https://pic1.zhimg.com/80/v2-3e68ff886a3ca434807814c01bac9654_720w.jpg" alt="img"></p><h2 id="03-总结"><a href="#03-总结" class="headerlink" title="03 总结"></a>03 <strong>总结</strong></h2><p>最后提供给对医学影像处理感兴趣的童鞋一个超级赞的github链接如下：</p><p><a href="https://link.zhihu.com/?target=https%3A//github.com/beamandrew/medical-data">https://github.com/beamandrew/medical-data</a></p><p>这是Github上哈佛 beamandrew机器学习和医学影像研究者贡献的数据集，包括了医学影像数据、竞赛数据、来自电子健康记录的数据、医疗数据、UCI数据集、生物医学文献等。</p><p><img src="https://pic4.zhimg.com/80/v2-6e6c12e74ff30c824bd4b929255a4a5f_720w.jpg" alt="img"></p><p><img src="https://pic2.zhimg.com/80/v2-2c1015e1042409b8f6aba1dd3bc2ccf1_720w.jpg" alt="img"></p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p><img src="https://i.loli.net/2020/12/18/5eiwYJNzb81ghLm.png" alt="放射组学中用到的一些库"></p><p>[5] H. J. Aerts et al., “Decoding tumour phenotype by noninvasive imaging using a quanti- tative radiomics approach,” Nat. Commun., vol. 5, June 2014. doi: 10.1038/ncomms5006.</p><p>[27] S. Armato et al., “Data from LIDC-IDRI: The cancer imaging archive.” doi: 10.7937/K9/TCIA.2015.LO9QL9SX.</p><p>[28] Z. Akkus, I. Ali, J. Sedlar, J. P. Agrawal, I. F. Parney, C. Giannini, and B.J. Erickson, “Predicting deletion of chromosomal arms 1p/19q in low-grade gliomas from MR images using machine intelligence,” J. Dig. Imaging, vol. 30, no. 4, pp. 469–476, 2017.</p><p>[29] M. Vallieres et al., “Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer,” Sci. Rep., vol. 7, no. 1, 2017. doi: 10.1038/s41598-017-10371-5.</p><p>[30] B. Menze et al., “The multimodal brain tumor image segmentation benchmark (BRATS),”(BRATS),” IEEE Trans. Med. Imag., vol. 34, no. 10, pp. 1993–2024, 2015.</p>]]></content>
    
    
    <categories>
      
      <category>medical imaging</category>
      
    </categories>
    
    
    <tags>
      
      <tag>medical imaging</tag>
      
      <tag>database</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>The first blog about Hexo</title>
    <link href="/2020/11/15/1st-blog-hexo/"/>
    <url>/2020/11/15/1st-blog-hexo/</url>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>&#8195;&#8195;选择基于github pages服务托管blog，利用hexo做blog框架，没有任何技术的原因，只因我的ex-boyfriend在一年前用它们搭建了他自己的技术blog，今年国庆长假后，他提前没有任何征兆提出的分手让我措手不及，多少次在睡梦中哭醒，想他，可我却狠心不联系他，微信、QQ和手机全部拉黑。但是我心痛，因此我想起做一个我自己的blog，换种方式在虚拟世界，与他隔空对话。只是他不知道我是谁。</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><blockquote><ul><li>有一个Github账号</li><li><p>安装了node.js，npm</p></li><li><p>安装了git（windows客户端）</p></li></ul></blockquote><h2 id="Hexo简介"><a href="#Hexo简介" class="headerlink" title="Hexo简介"></a>Hexo简介</h2><p>&#8195;&#8195;Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。大家可以进入<a href="https://hexo.io/zh-cn/">hexo官网</a>进行详细查看。</p><p>&#8195;&#8195;本安装参考其他博主们安装教程，整理成三个部分：</p><ul><li>第一部分：Hexo的初级搭建，并部署到github page上，以及个人域名的绑定；</li><li>第二部分：Hexo的基本配置，更换主题；(未完待续)</li><li>第三部分：Hexo添加各种功能，包括搜索的SEO、阅读量统计、访问量统计和评论系统等。(未完待续)</li></ul><h1 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h1><p>&#8195;&#8195;Hexo的初级搭建，并部署到github page上，以及个人域名的绑定。（其实在认识我的ex-boyfriend之前，我是个程序小白，虽然知道一点数据结构，入门的C语言和python。但基本属于最多看懂代码的程度。认识他以后，我可以用C++改写代码跑模拟实验；会用Linux操作系统，简单地使用命令行在Linux上做一些基本操作；用LaTeX编辑论文；用Markdown做笔记；还有Mendeley收集整理论文、Dropbox云、Teamviewer远程……那个时候，我总像一个小迷妹一样，看着那个安静的他用手指敲击着键盘，然后变魔术一般的变出很多有意思的东西来）。不小心又开始回忆了，感慨一句一切已成空，希望几年以后我们还可以再续前缘。</p><h2 id="Hexo搭建步骤"><a href="#Hexo搭建步骤" class="headerlink" title="Hexo搭建步骤"></a>Hexo搭建步骤</h2><ol><li><p>安装Git</p></li><li><p>安装Node.js</p></li><li><p>安装Hexo</p></li><li><p>Github创建个人仓库（resposity）</p></li><li><p>生成SSH添加到Github</p></li><li><p>将Hexo部署到Github</p></li><li><p>设置个人域名</p></li><li><p>发布文章</p><h3 id="1-安装Git"><a href="#1-安装Git" class="headerlink" title="1. 安装Git"></a>1. 安装Git</h3><p>&#8195;&#8195; Git是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的Hexo博客文章，上传到GitHub的工具。Git非常强大，我觉得建议每个人都去了解一下。廖雪峰老师的Git教程写的非常好，大家可以了解一下。<a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000">Git教程</a></p><ul><li><p>windows：到git官网上下载<a href="https://gitforwindows.org/">Download git</a>,下载后会有一个Git Bash的命令行工具，以后就用这个工具来使用git。双击下载好的exe文件，一路next就好。</p></li><li><p>linux：对linux来说实在是太简单了，因为最早的git就是在linux上编写的，只需要一行代码</p></li></ul><pre><code class="hljs routeros">sudo apt-<span class="hljs-builtin-name">get</span> install git</code></pre><p>安装好后，用<code>git --version</code> 来查看一下版本</p><h3 id="2-安装Node-js"><a href="#2-安装Node-js" class="headerlink" title="2.安装Node.js"></a>2.安装Node.js</h3><p>&#8195;&#8195;Hexo是基于nodejs编写的，所以需要安装一下nodeJs和里面的包管理npm工具。Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境。 Nodejs 使用了一个事件驱动、非阻塞式 I/O 的模型。</p><ul><li><p>windows：<a href="http://nodejs.cn/download/">nodejs</a>选择LTS版本就行了。</p><p><strong>安装步骤</strong>：下载后msi文件后，双击打开安装，一路next，在Custom Setup这一步记得选 <code>Add to PATH</code> ,这样就不用自己去配置电脑上环境变量了，装完后用Git Bash的命令行工具或按 <code>win + r</code> 快捷键调出运行，然后输入cmd确定，在cmd中输入path可以看到你的node是否配置在里面（环境变量）。</p><p><strong>查看版本</strong>：命令 <code>node -v</code></p></li><li><p>linux:</p><pre><code class="hljs routeros">1. sudo apt-<span class="hljs-builtin-name">get</span> install nodejs2. sudo apt-<span class="hljs-builtin-name">get</span> install npm</code></pre><p>安装完后，打开命令行</p><pre><code class="hljs crmsh"><span class="hljs-number">1</span>. <span class="hljs-keyword">node</span> <span class="hljs-title">-v</span><span class="hljs-number">2</span>. npm -v</code></pre></li></ul></li></ol><h3 id="3-安装Hexo"><a href="#3-安装Hexo" class="headerlink" title="3. 安装Hexo"></a>3. 安装Hexo</h3><p>&#8195;&#8195;前面git和nodejs安装好后，就可以安装Hexo了，你可以先创建一个文件夹blog，然后cd到这个文件夹下（或者在这个文件夹下直接右键git bash打开）。</p><p>输入命令</p><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span></code></pre><p>依旧利用<code>hexo -v</code>查看一下版本</p><p>接下来初始化一下hexo</p><pre><code class="hljs csharp">hexo <span class="hljs-keyword">init</span> myblog</code></pre><p>这个myblog可以自己取什么名字都可以，然后</p><pre><code class="hljs angelscript"><span class="hljs-number">1.</span> cd myblog <span class="hljs-comment">//进入这个myblog文件夹</span><span class="hljs-number">2.</span> npm install</code></pre><p>新建完成后，指定文件夹目录下有：</p><ul><li><p>node_modules: 依赖包</p></li><li><p>public：存放生成的页面</p></li><li><p>scaffolds：生成文章的一些模板</p></li><li><p>source：用来存放你的文章</p></li><li><p>themes：主题</p></li><li><p>_config.yml: 博客的配置文件</p> <pre><code class="hljs verilog"><span class="hljs-number">1</span>. hexo <span class="hljs-keyword">generate</span> <span class="hljs-comment">// 或用缩写g</span><span class="hljs-number">2</span>. hexo server</code></pre></li></ul><p>打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。大概长这样：<br>   <img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrksvj6e0j211c0f2n60.jpg" alt=""><br>使用ctrl+c可以把服务关掉。</p><h3 id="4-Github创建个人仓库"><a href="#4-Github创建个人仓库" class="headerlink" title="4. Github创建个人仓库"></a>4. Github创建个人仓库</h3><p>&#8195;&#8195;首先，你先要有一个GitHub账户。在GitHub.com中看到一个New repository，新建仓库创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名。（此处引用别人的图，我偷个懒）</p><p><img src="https://i.loli.net/2020/11/14/stg6iZuDJ4OrWBY.jpg" alt=""></p><p>点击create repository。</p><p>另外要注意的是braches 要将master要设为默认，否则会切换到main。GitHub Pages 要将Source的Brach设为master。</p><h3 id="5-生成SSH添加到GitHub"><a href="#5-生成SSH添加到GitHub" class="headerlink" title="5.生成SSH添加到GitHub"></a>5.生成SSH添加到GitHub</h3><p>回到git bash中，</p><pre><code class="hljs routeros">1. git<span class="hljs-built_in"> config </span>--global user.name <span class="hljs-string">&quot;yourname&quot;</span>2. git<span class="hljs-built_in"> config </span>--global user.email <span class="hljs-string">&quot;youremail&quot;</span></code></pre><p>这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。可以用以下两条，检查一下你有没有输对</p><pre><code class="hljs routeros">1. git<span class="hljs-built_in"> config </span>user.name2. git<span class="hljs-built_in"> config </span>user.email</code></pre><p>然后创建SSH,一路回车</p><pre><code class="hljs excel">ssh-keygen -<span class="hljs-built_in">t</span> rsa -C <span class="hljs-string">&quot;youremail&quot;</span></code></pre><p>这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹。（继续借用别人的图）</p><p><img src="https://i.loli.net/2020/11/14/JbjR6iD4dFAlgS5.jpg" alt=""></p><p>&#8195;&#8195;ssh，简单来讲，就是一个秘钥，其中，<code>id_rsa</code>是你这台电脑的私人秘钥，不能给别人看的，<code>id_rsa.pub</code>是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。而后在GitHub的setting中，找到SSH keys的设置选项，点击<code>New SSH key</code>，把你的<code>id_rsa.pub</code>里面的信息复制进去。</p><p>在git bash中，查看是否成功</p><pre><code class="hljs nginx"><span class="hljs-attribute">ssh</span> -T git<span class="hljs-variable">@github</span>.com</code></pre><h3 id="6-将Hexo部署到GitHub"><a href="#6-将Hexo部署到GitHub" class="headerlink" title="6.将Hexo部署到GitHub"></a>6.将Hexo部署到GitHub</h3><p>&#8195;&#8195;这一步，我们就可以将Hexo和GitHub关联起来，也就是将Hexo生成的文章部署到GitHub上，打开站点配置文件 <code>_config.yml</code>，翻到最后，修改为YourgithubName就是你的GitHub账户</p><pre><code class="hljs less"><span class="hljs-attribute">deploy</span>:  <span class="hljs-attribute">type</span>: git  <span class="hljs-attribute">repo</span>: git<span class="hljs-variable">@github</span>.<span class="hljs-attribute">com</span>:tiamoqin/tiamoqin.github.io.git  <span class="hljs-attribute">branch</span>: master</code></pre><p> 这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。</p><pre><code class="hljs sql">npm <span class="hljs-keyword">install</span> hexo-deployer-git <span class="hljs-comment">--save</span></code></pre><p>然后</p><pre><code class="hljs verilog"><span class="hljs-number">1</span><span class="hljs-variable">.hexo</span> clean<span class="hljs-number">2</span><span class="hljs-variable">.hexo</span> <span class="hljs-keyword">generate</span><span class="hljs-number">3</span><span class="hljs-variable">.hexo</span> deploy</code></pre><p>其中 <code>hexo clean</code>清除了你之前生成的东西，也可以不加。<br><code>hexo generate</code> 顾名思义，生成静态文章，可以用 <code>hexo g</code>缩写<br><code>hexo deploy</code> 部署文章，可以用<code>hexo d</code>缩写</p><p>注意deploy时可能要你输入username和password。</p><p>过一会儿就可以在<code>http://yourname.github.io</code> 这个网站看到你的博客了！！</p><h3 id="7-设置个人域名"><a href="#7-设置个人域名" class="headerlink" title="7. 设置个人域名"></a>7. 设置个人域名</h3><p>&#8195;&#8195;现在你的个人网站的地址是 <code>yourname.github.io</code>，如果觉得这个网址逼格不太够，这就需要你设置个人域名了。但是需要花钱。在<a href="https://sg.godaddy.com/">godaddy</a>和<a href="https://wanwang.aliyun.com/?spm=5176.8142029.digitalization.2.e9396d3e46JCc5">阿里云</a>上买一个域名，我买的是 <code>tiamoqin.me</code>，各个后缀的价格不太一样。</p><p>&#8195;&#8195;以下示范，是阿里云域名网站。点<strong>解析</strong>进去，添加解析。godaddy请参考[<a href="https://www.cnblogs.com/openxxs/p/5950598.html">通过GitHub和GoDaddy搭建静态个人博客</a>]，godaddy的交互很坑人，我找了半天的Domains manage。</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrkstcu8xj20d607wdfw.jpg" alt=""></p><p>其中，192.30.252.153 和 192.30.252.154 是GitHub的服务器地址，或从这里<a href="https://docs.github.com/en/free-pro-team@latest/github/working-with-github-pages/managing-a-custom-domain-for-your-github-pages-site">获取服务器地址</a><br><strong>注意，解析线路选择默认</strong>。这个境外是后面来做国内外分流用的,在后面的博客中会讲到。记得现在选择<strong>默认</strong>！！</p><p><img src="http://ww1.sinaimg.cn/large/d40b6c29gy1fvrkstf8unj20ob05b0sq.jpg" alt=""></p><p>登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名<code>tiamoqin.me</code></p><p>然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。</p><p><img src="https://i.loli.net/2020/11/14/REtsF6WG3vknq8X.png" alt=""></p><p>最后，在gitbash中，输入</p><pre><code class="hljs angelscript"><span class="hljs-number">1.</span> hexo clean<span class="hljs-number">2.</span> hexo g<span class="hljs-number">3.</span> hexo d</code></pre><p>过不了多久，再打开你的浏览器，输入你自己的域名，就可以看到搭建的网站啦！</p><p>接下来你就可以正式开始写文章了。</p><pre><code class="hljs actionscript">hexo <span class="hljs-keyword">new</span> newpapername</code></pre><p>然后在source/_post中打开markdown文件，就可以开始编辑了。当你写完的时候，再</p><pre><code class="hljs angelscript"><span class="hljs-number">1.</span> hexo clean<span class="hljs-number">2.</span> hexo g<span class="hljs-number">3.</span> hexo d</code></pre><p>或者省略形式</p><pre><code class="hljs awk">hexo s -g <span class="hljs-regexp">//</span>生成并启动本低服务器hexo d -g <span class="hljs-regexp">//</span>生成并部署到github</code></pre><p>就可以看到更新了。注意由于域名和服务器分属不同厂商，DNS缓存有点慢，打开域名，好久才可以更新。大概十几分钟。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><p><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029">hexo史上最全搭建教程</a></p></li><li><p><a href="http://blog.haoji.me/build-blog-website-by-hexo-github.html">使用hexo+github搭建免费个人博客详细教程</a></p></li><li><p><a href="https://www.jianshu.com/p/05289a4bc8b2">如何搭建一个独立博客——简明Github Pages与Hexo教程</a></p></li><li><p><a href="https://www.cnblogs.com/zhcncn/p/4097881.html">Hexo搭建Github静态博客</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>blog</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
