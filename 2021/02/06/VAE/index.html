

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/cat.jpg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <title>VAE - Hexo</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.1.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"copy_btn":true,"image_zoom":{"enable":true},"lazyload":{"enable":true,"onlypost":false},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Tiamome</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner intro-2" id="background" parallax=true
         style="background: url('/img/maskgirl.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="VAE">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-02-06 21:43" pubdate>
        2021年2月6日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.9k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      64
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">VAE</h1>
            
            <div class="markdown-body">
              <h1 id="变分自编码器（Variational-Autoencoder，VAE"><a href="#变分自编码器（Variational-Autoencoder，VAE" class="headerlink" title="变分自编码器（Variational Autoencoder，VAE)"></a>变分自编码器（Variational Autoencoder，VAE)</h1><p>VAE的整理准备了三天，最近可能在家窝的太久了，总是有点昏昏欲睡，结果听了“宝可梦”老师的VAE，更是加速了我的困意。（其实身体的不精神，也会让自己的心情不美妙，和Mr. Duan微聊以后，心情好很多。）</p>
<p>在CSDN上有“宝可梦”老师的上课笔记，<a target="_blank" rel="noopener" href="https://blog.csdn.net/dugudaibo/category_7336223.html">机器学习课程笔记完结</a>,其中本章节在<a target="_blank" rel="noopener" href="https://blog.csdn.net/dugudaibo/article/details/79078264">无监督学习：深度自编码器</a>和<a target="_blank" rel="noopener" href="https://blog.csdn.net/dugudaibo/article/details/79085412">无监督学习：生成模型</a></p>
<p>在开始VAE之前，先简单介绍下自编码器和其分类。</p>
<h2 id="1-自编码器（Auto-encoder）"><a href="#1-自编码器（Auto-encoder）" class="headerlink" title="1. 自编码器（Auto encoder）"></a>1. 自编码器（Auto encoder）</h2><p>编码器为从“旧特征”表示中产生“新特征”表示（通过选择或提取）的过程，然后将其逆过程称为解码。降维可以被理解为数据压缩，其中编码器压缩数据（从初始空间到<strong>编码空间</strong>，也称为<strong>隐空间</strong>，latent sapce），而解码器则用于解压缩。当然，根据初始数据分布、隐空间大小和编码器的选择，压缩可能是有损的，即一部分信息会在编码过程中丢失，并且在解码时无法恢复。</p>
<p>自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的。在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。</p>
<p>1）<strong>自动编码器是数据相关的（data-specific 或 data-dependent），这意味着自动编码器只能压缩那些与训练数据类似的数据</strong>。比如，使用人脸训练出来的自动编码器在压缩别的图片，比如树木时性能很差，因为它学习到的特征是与人脸相关的。</p>
<p>2）<strong>自动编码器是有损的，意思是解压缩的输出与原来的输入相比是退化的</strong>。</p>
<p>3）<strong>自动编码器是从数据样本中自动学习的</strong>，这意味着很容易对指定类的输入训练出一种特定的编码器，而不需要完成任何新工作。</p>
<p>基本上，要求模型在像素级上精确重构输入不是机器学习的兴趣所在，学习到高级的抽象特征才是。事实上，当主要任务是分类、定位之类的任务时，那些对这类任务而言的最好的特征基本上都是重构输入时的最差的那种特征。目前自编码器的应用主要有两个方面，<strong>第一是数据去噪，第二是为进行可视化而降维</strong>。<strong>配合适当的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。</strong></p>
<p><strong>对于2D的数据可视化，t-SNE或许是目前最好的算法，</strong>但通常还是需要原数据的维度相对低一些。所以，可视化高维数据的一个好办法是<strong>首先使用自编码器将维度降低到较低的水平（如32维），然后再使用t-SNE将其投影在2D平面上。</strong></p>
<p>自编码器（autoencoder）是神经网络的一种，经过训练后能尝试将输入复制到输出。自编码器（autoencoder）内部有一个隐藏层 h，可以产生编码（code）表示输入。该网络可以看作由两部分组成：一个由函数 h = f(x) 表示的编码器和一个生成重构的解码器 r = g(h)。如果一个自编码器只是简单地学会将处处设置为 g(f(x)) = x，那么这个自编码器就没什么特别的用处。相反，我们不应该将自编码器设计成输入到输出完全相等。这通常需要向自编码器强加一些约束，使它只能近似地复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。</p>
<p><img src="https://i.loli.net/2021/02/07/GmF6JS5spvTtqaM.png" srcset="/img/loading.gif" alt="Autoencoder-architecture.png"></p>
<p>自编码器和前馈神经网络的比较</p>
<p>二者的区别和联系如下：</p>
<blockquote>
<p>（1）自编码器是前馈神经网络的一种，最开始主要用于数据的降维以及特征的抽取，随着技术的不断发展，现在也被用于生成模型中，可用来生成图片等。<br>（2）前馈神经网络是有监督学习，其需要大量的标注数据。自编码器是无监督学习（自对比），数据不需要标注因此较容易收集。<br>（3）前馈神经网络在训练时主要关注的是输出层的数据以及错误率，而自编码的应用可能更多的关注中间隐层的结果。</p>
</blockquote>
<p>先听的是”宝可梦”老师2016年的机器学习课程，感叹AI技术更新飞快，关于自编码器这一讲中的深度自编码器（Deep Autoencoder）在如今的知乎上已很少见，关于利用深度自编码器数据预训练（pre-training)的方法已经被新的方法所替代。但深层网络的训练思想保留下来，总体归结为：无监督预训练，有监督微调（fine-tuning）。</p>
<h2 id="2-几种自编码器"><a href="#2-几种自编码器" class="headerlink" title="2. 几种自编码器"></a>2. 几种自编码器</h2><h3 id="2-1-欠完备自编码器"><a href="#2-1-欠完备自编码器" class="headerlink" title="2.1 欠完备自编码器"></a>2.1 欠完备自编码器</h3><p>通过训练自编码器对输入进行复制而使得$h$获得有用的特征。如果从自编码器获得有用特征的一种方法是限制$h$的维度比$x$小，这种编码维度小于输入维度的自编码器称为<strong>欠完备自编码器(undercomplete),</strong>学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。学习的过程就是最小化重构误差.如果当解码器是线性的且$L$是均方误差，欠完备自编码器会学习出与PCA相同的生成子空间。因此，拥有非线性编码器函数$f$和非线性解码器函数$g$的自编码器能够学习出更加强大的PCA非线性推广，但不幸的是，如果编码器和解码器被赋予过大的容量，自编码器会执行赋值任务而捕捉不到任何有关数据分布的有用信息。</p>
<h4 id="2-1-1-香草自编码器"><a href="#2-1-1-香草自编码器" class="headerlink" title="2.1.1 香草自编码器"></a>2.1.1 香草自编码器</h4><p>在这里，隐含层维数（1024）小于输入维数（10000），则称这个编码器是有损的。通过这个约束，来迫使神经网络来学习数据的压缩表征。</p>
<p>网络结构举例：$10000 \to 1024 \to 10000$</p>
<h4 id="2-1-2-多层自编码器"><a href="#2-1-2-多层自编码器" class="headerlink" title="2.1.2 多层自编码器"></a>2.1.2 多层自编码器</h4><p>显然可以将自动编码器的隐含层数目增加。在这里使用了3个隐含层，<strong>任意一个隐含层都可以作为特征表征。</strong>多层编码器由多个自动编码器串联组成，能够逐层提取输入数据的特征，在此过程中逐层降低数据的维度，将高维数据数据转化为低维特征。</p>
<p>网络结构举例：$10000 \to 4096 \to 1024 \to 4096 \to 10000$</p>
<h4 id="2-1-3卷积自编码器"><a href="#2-1-3卷积自编码器" class="headerlink" title="2.1.3卷积自编码器"></a>2.1.3卷积自编码器</h4><h3 id="2-2-正则自编码器"><a href="#2-2-正则自编码器" class="headerlink" title="2.2 正则自编码器"></a>2.2 正则自编码器</h3><p>其实，除了除了施加一个比输入维度小的隐含层，一些其他方法也可用来约束自编码器重构，比如<strong>正则编码器</strong>。它使用损失函数鼓励模型学习其他特性(除了将输入复制到输出)，而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量，这些特性包括稀疏表示，表示的小导数以及噪声或输入缺失的鲁棒性。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则编码器仍然能够从中学习到一些关于数据分布的有用信息。在实际应用中常用的两种正则编码器：稀疏编码器和去噪编码器。</p>
<h4 id="2-2-1-稀疏自编码器"><a href="#2-2-1-稀疏自编码器" class="headerlink" title="2.2.1 稀疏自编码器"></a>2.2.1 稀疏自编码器</h4><p>稀疏自编码器简单地在训练时结<strong>合编码层的稀疏惩罚 $Ω(h)$</strong> 和重构误差：$L(x,g(f(x))) + Ω(h)$，其中$g(h)$是解码器的输出，通常$h$是编码器的输出，即$h = f(x)$。<strong>稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。稀疏正则化的自编码器必须反映训练数据集的独特统计特征，而不是简单地充当恒等函数</strong>。以这种方式训练，执行附带稀疏惩罚的复制任务可以得到能<strong>学习有用特征</strong>的模型。</p>
<h4 id="2-2-2-去噪自编码器"><a href="#2-2-2-去噪自编码器" class="headerlink" title="2.2.2 去噪自编码器"></a>2.2.2 去噪自编码器</h4><p>除了向代价函数增加一个惩罚项，还可以通过改变重构误差来获得一个能学习到有用信息的编码器。在训练样本中加入随机噪声，重构的目标使不带噪声的样本数据。</p>
<p>对于每个样本向量$x$随机选择其中的部分分量，将它们的值置为0，其他分量保持不变，得到的带噪声向量为 $\hat x$，<strong>去噪自编码器(denoising autoencoder, DAE)</strong>最小化.其中$\hat x$是被某种噪声损坏的$x$的副本。向训练数据加入噪声，并使自编码器学会去除这种噪声来获得没有被噪声污染过的真实输入。因此，这就迫使编码器学习提取最重要的特征并学习输入数据中更加鲁棒的表征。</p>
<h4 id="2-2-3-收缩自编码器"><a href="#2-2-3-收缩自编码器" class="headerlink" title="2.2.3 收缩自编码器"></a>2.2.3 收缩自编码器</h4><p>为了提高对训练集数据点周围小扰动的鲁棒性，<strong>收缩自编码器(Contractive autoencoder, CAE)</strong>在基础自编码器上添加了正则项，另一个正则化自编码器的策略是使用一个类似系数编码器中的惩罚项 $\Omega$，但$\Omega$的形式不同：</p>
<script type="math/tex; mode=display">
L(x,g(f(x)))+\Omega(h,x)\\
\Omega(h,x)=\lambda \sum_i||\nabla_xh_i||^2
\tag 1</script><p>这迫使模型学习一个在x变化小时目标没有太大变化的函数，因为这个惩罚只对训练数据适用，它迫使自编码器学习可以反映数据分布信息的特征。这种正则编码器称为收缩编码器(Contractinve autoencoder, CAE)。</p>
<p>收缩自编码器在编码$h=f(x)$的基础上添加了显式的正则项，鼓励$f$的导数尽可能地小，对输入不那么敏感：</p>
<script type="math/tex; mode=display">
\Omega(h)=\lambda||\cfrac{\partial f(x)}{\partial x}||_F^2
\tag 2</script><p>惩罚项$\Omega(h)$为平方$F$范数（元素平方之和），作用于与编码器的函数相关偏导数的雅克比矩阵。收缩源于CAE弯曲空间的方式，具体来说就是由于CAE训练为抵抗输入扰动，鼓励将输入点领域映射到输出点出更小的领域。CAE的目标使学习数据的流形结构。</p>
<h2 id="3-变分自编译器"><a href="#3-变分自编译器" class="headerlink" title="3. 变分自编译器"></a>3. 变分自编译器</h2><p>重头戏来了，研究了三天才搞明白。首先思考一些问题。</p>
<h3 id="3-1-自编码器用于内容生成的局限性"><a href="#3-1-自编码器用于内容生成的局限性" class="headerlink" title="3.1 自编码器用于内容生成的局限性"></a>3.1 自编码器用于内容生成的局限性</h3><p>“自编码器和内容生成之间的联系是什么？”。确实，一旦对自编码器进行了训练，我们既有编码器又有解码器，但是仍然没有办法来产生任何新内容。乍一看，我们可能会认为，如果隐空间足够规则（在训练过程中被编码器很好地“组织”了），我们可以从该隐空间中随机取一个点并将其解码以获得新的内容，就像生成对抗网络中的生成器一样。</p>
<p><img src="https://i.loli.net/2021/02/08/doilLXmM1TuctWa.png" srcset="/img/loading.gif" alt="我们可以通过解码从隐空间中随机采样的点来生成新数据。生成数据的质量取决于隐空间的规则性。【在生成任务中我们更关注解码器，生成器其实是训练过程中的辅助，就像GAN里的判别器一样】"></p>
<p>但是，自编码器的隐空间的规则性是一个难点，它取决于初始空间中数据的分布、隐空间的大小和编码器的结构。因此，很难先验地确保编码器以与刚刚描述的生成过程兼容的方式智能地组织隐空间。自编码器的高自由度使得可以在没有信息损失的情况下进行编码和解码（尽管隐空间的维数较低）但<strong>会导致严重的过拟合</strong>，这意味着隐空间的某些点将在解码时给出无意义的内容。</p>
<p><strong>自编码器仅以尽可能少的损失为目标进行训练，而不管隐空间如何组织</strong>。因此，如果我们对架构的定义不小心，那么在训练过程中，网络很自然地会利用任何过拟合的可能性来尽可能地完成其任务……除非我们明确对其进行规范化！</p>
<h3 id="3-2-变分自编码器的定义"><a href="#3-2-变分自编码器的定义" class="headerlink" title="3.2 变分自编码器的定义"></a>3.2 变分自编码器的定义</h3><p>因此，为了能够将我们的自编码器的解码器用于生成目的，必须确保隐空间足够规则。获得这种规律性的一种可能方案是在训练过程中引入显式的正规化（regularisation）。<strong>因此，变分自编码器可以定义为一种自编码器，其训练经过正规化以避免过度拟合，并确保隐空间具有能够进行数据生成过程的良好属性。</strong></p>
<p>就像标准自编码器一样，变分自编码器是一种由编码器和解码器组成的结构，经过训练以使编码解码后的数据与初始数据之间的重构误差最小。但是，为了引入隐空间的某些正则化，我们对编码-解码过程进行了一些修改：<strong>我们不是将输入编码为隐空间中的单个点，而是将其编码为隐空间中的概率分布</strong>。然后对模型进行如下训练：</p>
<ul>
<li>首先，将输入编码为在隐空间上的分布；</li>
<li>第二，从该分布中采样隐空间中的一个点；</li>
<li>第三，对采样点进行解码并计算出重建误差；</li>
<li>最后，重建误差通过网络反向传播。</li>
</ul>
<p><img src="https://i.loli.net/2021/02/08/VUlcONt8IbyQJvf.png" srcset="/img/loading.gif" alt="自编码器（确定性）和变分自编码器（概率性）之间的差异。【变分自编码器把原始数据编码为隐空间中的分布，在解码时是从该分布中采样一个点来进行解码】"></p>
<p>实践中，选择正态分布作为编码的分布，使得我们可以训练编码器来返回描述高斯分布的均值和协方差矩阵。将输入编码为具有一定方差而不是单个点的分布的原因是这样可以非常自然地表达隐空间规则化：编码器返回的分布被强制接近标准正态分布。在下一节中，我们将通过这种方式确保隐空间的局部和全局正则化（局部是由于方差控制，而全局是由于均值控制）。</p>
<p>因此，在训练VAE时最小化的损失函数由一个“重构项”（在最后一层）组成，“重构项”倾向于使编码解码方案尽可能地具有高性能，而一个“正则化项”（在隐层）通过使编码器返回的分布接近标准正态分布，来规范隐空间的组织。该正则化项为返回的分布与标准高斯之间的<strong>Kulback-Leibler散度</strong>，这将在下一节中进一步说明。我们可以注意到，两个高斯分布之间的$KL$散度具有封闭形式，可以直接用两个分布的均值和协方差矩阵表示。</p>
<p><img src="https://i.loli.net/2021/02/08/6JP3n91myN7aOhS.png" srcset="/img/loading.gif" alt="在变分自动编码器中，损失函数由一个重构项（优化编码-解码）和一个正则化项（使隐空间规则化）组成。【小提示，由于正态分布的概率密度函数是确定的，所以两个高斯分布的KL散度有解析形式】"></p>
<h4 id="3-2-1关于正则化的直观解释"><a href="#3-2-1关于正则化的直观解释" class="headerlink" title="3.2.1关于正则化的直观解释"></a>3.2.1关于正则化的直观解释</h4><p>了使生成过程成为可能，我们期望隐空间具有规则性，这可以通过两个主要属性表示：<strong>连续性</strong>（continuity，隐空间中的两个相邻点解码后不应呈现两个完全不同的内容）和<strong>完整性</strong>（completeness，针对给定的分布，从隐空间采样的点在解码后应提供“有意义”的内容）。</p>
<p><img src="https://i.loli.net/2021/02/08/laSbxjIfRY61dvt.png" srcset="/img/loading.gif" alt="不规范的隐空间vs规范的隐空间。【不规范隐空间中临近的点解码后不相似（正方形和三角形），而且有的点无法解析出有意义的数据（紫色图形）】"></p>
<p>VAE将输入编码为分布而不是点，不足以确保连续性和完整性。如果没有明确定义的正则化项，则模型可以学习最小化其重构误差，从而“忽略”要返回一个分布，最终表现得几乎像普通自编码器一样（导致过度拟合）。具体地说，编码器可以返回具有微小方差的分布（往往是点分布，punctual distributions），或者返回具有巨大均值差异的分布（数据在隐空间中彼此相距很远）。在这两种情况下，返回分布的限制都没有取得效果，并且不满足连续性和/或完整性。</p>
<p>因此，为了避免这些影响，<strong>我们必须同时对协方差矩阵和编码器返回的分布均值进行正则化</strong>。实际上，通过强制分布接近标准正态分布（集中和简化）来完成此正则化。这样，我们要求协方差矩阵接近于单位阵，防止出现单点分布，并且均值接近于0，防止编码分布彼此相距太远。</p>
<p><img src="https://i.loli.net/2021/02/08/oQeImGfgbt8351r.png" srcset="/img/loading.gif" alt="必须对返回的VAE分布进行正则化，以获得具有良好属性的隐空间"></p>
<p>使用此正则化项，我们可以防止模型在隐空间中的编码相互远离，并鼓励尽可能多的返回分布发生“重叠”，从而满足预期的连续性和完整性条件。自然地，对于任何正则化项，都会以训练数据上更高的重建误差为代价。然而，可以调整重建误差和KL散度之间的权重，我们将在下一节中看到如何从形式推导中自然得出平衡的表达。</p>
<p>总结这一小节，我们可以观察到，通过正则化获得的连续性和完整性往往会在隐空间中编码的信息上产生“梯度”。例如，应将隐空间中位于来自不同训练数据的两个编码分布的均值的中间点应该被解码为提供第一个分布的数据和提供第二个分布的数据之间的某个数据。因为它可能在两种情况下被自编码器采样到【这句话有点绕，我理解是解码之前是从分布采样，所以可能会采样到这种“重叠区域”里的点，我们希望这种点解码出来会跟任意一种原始数据比较像】。</p>
<h4 id="3-2-2-变分自编译器的变分推理过程"><a href="#3-2-2-变分自编译器的变分推理过程" class="headerlink" title="3.2.2 变分自编译器的变分推理过程"></a>3.2.2 变分自编译器的变分推理过程</h4><p>关于变分推理过程，“宝可梦”老师的上课视频结合材料多听两次，就很明白了，在这里我就不记录了。以下是转自于苏剑林的<a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/5253">变分自编码器（一）：原来是这么一回事 </a></p>
<p>在VAE模型中，<strong>我们并没有去使用$p(z)$（隐变量空间的分布）是正态分布的假设，我们用的是假设$p(z|x)$(后验分布）是正态分布！！</strong></p>
<p>具体来说，给定一个真实样本$x_k$，我们假设存在<strong>一个专属于$x_k$的分布$p(z|x_k)$</strong>（学名叫后验分布），并进一步假设这个分布是（独立的、多元的）正态分布。为什么要强调“专属”呢？因为我们后面要训练一个生成器$x=g(z)$，希望能够把从分布$p(z|x_k)$采样出来的一个$z_k$还原为$x_k$。如果假设$p(z)$是正态分布，然后从$p(z)$中采样一个$z$，那么我们怎么知道这个$z$对应于哪个真实的$x$呢？<strong>现在$p(z|x_k)$专属于$x_k$，我们有理由说从这个分布采样出来的$z$应该要还原到$x_k$中去。</strong>即</p>
<script type="math/tex; mode=display">
\log q(z|x_k)=\log \boldsymbol{N}(z;\mu_k,\sigma_k^2I)
\tag 3</script><p>这时候每一个$x_k$都配上了一个专属的正态分布，才方便后面的生成器做还原。但这样有多少个$x$就有多少个正态分布了。我们知道正态分布有两组参数：均值$\mu$和方差$\sigma^2$（多元的话，它们都是向量），那我怎么找出专属于$x_k$的正态分布$p(z|x_k)$的均值和方差呢？好像并没有什么直接的思路。那好吧，那就用神经网络来拟合出来。（此处省略了部分内容，可参考原文）</p>
<p>为了避免模型退化成普通的自编码器，VAE让所有的$p(z|x)$都像标准正态分布看齐，这样就防止了噪声为零。即</p>
<script type="math/tex; mode=display">
p(z)=\sum_xp(z|x)p(x)=\sum_x \boldsymbol{N}(0,I)p(x)=\boldsymbol{N}(0,I)\sum_xp(x)=\boldsymbol{N}(0,I)
\tag 4</script><p>这样就能达到我们的先验假设：$p(z)$是标准正态分布。然后我们就可以放心地从$N(0,I)$中采样来生成图像了。</p>
<p>那如何使得$p(z|x)$都想标准正态分布开齐呢，利用分量独立的正态分布与标准正态分布的$KL$散度$KL(N(\mu,\sigma^2)∥N(0,I))$作为这个额外的loss，计算过程如下(写等式写到眼瞎）：</p>
<script type="math/tex; mode=display">
\begin{align}
KL(\boldsymbol{N}(\mu,\sigma^2)||\boldsymbol{N}(0,I))&=\int\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}(\log\cfrac
{e^{-(x-\mu)^2/2\sigma^2}/\sqrt{2\pi\sigma^2}}{e^{-x^2/2}/\sqrt{2\pi}})dx\\
& =\int\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\log\{\cfrac{1}{\sqrt{\sigma^2}}exp\{\cfrac{1}{2}[x^2-(x-\mu)^2/\sigma^2]\}\}dx\\
& =\cfrac{1}{2}\int\cfrac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}[-\log\sigma^2+x^2-(x-\mu)^2/\sigma^2]dx\\
\end{align}
\tag 5</script><blockquote>
<p>整个结果分为三项积分：</p>
<ul>
<li>第一项为$-\log\sigma^2$乘以概率密度的积分（也就是1），所以结果是$-\log \sigma^2$;</li>
<li>第二项是正态分布的二阶矩，所以结果为$\mu^2+\sigma^2$;</li>
<li>第三项实际就是“-方差除以方差=-1”</li>
</ul>
</blockquote>
<p>因此（5）式的结果为</p>
<script type="math/tex; mode=display">
KL(\boldsymbol{N}(\mu,\sigma^2)||\boldsymbol{N}(0,I))=\cfrac{1}{2}(-\log\sigma^2+\mu^2+\sigma^2-1)
\tag 6</script><h4 id="3-2-3-重参数技巧"><a href="#3-2-3-重参数技巧" class="headerlink" title="3.2.3 重参数技巧"></a>3.2.3 重参数技巧</h4><p>最后是实现模型的一个技巧，英文名是<strong>reparameterization trick</strong>，我这里叫它做重参数吧。其实很简单，就是我们要从$p(z|x_k)$中采样一个$z_k$出来，尽管我们知道了$p(z|x_k)$是正态分布，但是均值方差都是靠模型算出来的，我们要靠这个过程反过来优化均值方差的模型，但是“采样”这个操作是不可导的，而采样的结果是可导的。我们利用</p>
<script type="math/tex; mode=display">
\cfrac{1}{\sqrt{2\pi\sigma^2}}exp(-\cfrac{(z-\mu)^2}{2\sigma^2})dz
=\cfrac{1}{\sqrt{2\pi}}exp[-\cfrac{1}{2}(\cfrac{z-\mu}{\sigma})^2]d(\cfrac{z-\mu}{\sigma})
\tag 7</script><p>这说明$\cfrac{z-\mu}{\sigma}$是服从均值为0、方差为1的标准正态分布的，要同时把$dz$考虑进去，是因为乘上$dz$才算是概率，去掉$dz$是概率密度而不是概率。这时候我们得到：从$N(\mu,\sigma^2)$中采样一个$z$，相当于从$N(0,I)$中采样一个$\epsilon$，然后让$z=\mu+\epsilon*\sigma$。这样一来，“采样”这个操作就不用参与梯度下降了，改为采样的结果参与，使得整个模型可训练了。</p>
<p>（今天的公式写到眼花缭乱，加速催眠，明天再核对吧。）</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dugudaibo/category_7336223.html">机器学习课程笔记完结</a>,</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dugudaibo/article/details/79078264">无监督学习：深度自编码器</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/51960803">自编码器分类及核心原理 - 石疯的文章 - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/84533223">深度学习(一)自动编码器 - 小小的文章 - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/dugudaibo/article/details/79085412">无监督学习：生成模型</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/144649293">半小时理解变分自编码器 - 不刷知乎的文章 - 知乎</a></li>
<li><a target="_blank" rel="noopener" href="https://spaces.ac.cn/archives/5253">苏剑林. (Mar. 18, 2018). 《变分自编码器（一）：原来是这么一回事 》</a></li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/machine-learning/">machine learning</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/machine-learning/">machine learning</a>
                    
                      <a class="hover-with-bg" href="/tags/unsupervised-learning/">unsupervised learning</a>
                    
                      <a class="hover-with-bg" href="/tags/dimension-reduction/">dimension reduction</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/02/05/t-SNE/">
                        <span class="hidden-mobile">t-SNE</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  

  
</footer>

<!-- SCRIPTS -->

  <script  src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":200})
    NProgress.start()
    document.addEventListener('DOMContentLoaded', function() {
      window.NProgress && window.NProgress.inc();
    })
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.staticfile.org/jquery/3.5.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  



  <script  src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js" ></script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/local-search.xml";
      var inputArea = document.querySelector("#local-search-input");
      inputArea.onclick = function () {
        searchFunc(path, 'local-search-input', 'local-search-result');
        this.onclick = null
      }
    })()
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.1.2/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>



</body>
</html>
